{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pyro\n",
    "from pyro.distributions import Categorical, Bernoulli\n",
    "from pyro import param\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import MCMC, NUTS\n",
    "from pyro.distributions import constraints\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import Predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_path = Path.cwd().parent / 'train_test_features.parquet'\n",
    "data = pd.read_parquet(feature_path) \n",
    "train, test = train_test_split(data, test_size=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(data):\n",
    "    '''\n",
    "    transforms continuous data in the range [0,1] and\n",
    "    no data values of  -99 to integers from 0 to 11.\n",
    "    Where 11 is no data value'''\n",
    "    # Handle the special case\n",
    "    special_case = (data == -99).int() * 11\n",
    "    discretized = (torch.clamp(torch.round(data * 10).int(), 0, 10) + special_case).long()\n",
    "    return discretized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ahh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features, found in mortality_classification.ipynb\n",
    "# based on abs(corr) < 0.4, and RF feature importance\n",
    "feature_names = [\n",
    "    'n_mean',\n",
    "    'lum70',\n",
    "    'savi_std',\n",
    "    'rgi_std',\n",
    "    'lum50',\n",
    "    'lum30',\n",
    "    'b_std',\n",
    "    'r_std',\n",
    "    'r40',\n",
    "    'lum40',\n",
    "    'g30',\n",
    "    'b50',\n",
    "    'lum10',\n",
    "    'b10',\n",
    "    'n10',\n",
    "    'rgi60',\n",
    "    'rgi80',\n",
    "    'r60',\n",
    "    'b80',\n",
    "    'rgi30',\n",
    "    'r70',\n",
    "    'n80',\n",
    "    'b60'\n",
    " ]\n",
    "\n",
    "features = torch.tensor(\n",
    "    train[feature_names].values,\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "labels = torch.tensor(\n",
    "    train.y.values,\n",
    "    dtype=torch.long\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 576205.5454101562\n",
      "Epoch 100 Loss: 576202.1416015625\n",
      "Epoch 200 Loss: 576197.7280273438\n",
      "Epoch 300 Loss: 576203.0209960938\n",
      "Epoch 400 Loss: 576202.9204101562\n",
      "Epoch 500 Loss: 576196.40234375\n",
      "Epoch 600 Loss: 576204.3061523438\n",
      "Epoch 700 Loss: 576207.6064453125\n",
      "Epoch 800 Loss: 576205.3486328125\n",
      "Epoch 900 Loss: 576201.6591796875\n",
      "Epoch 1000 Loss: 576206.0385742188\n",
      "Epoch 1100 Loss: 576201.205078125\n",
      "Epoch 1200 Loss: 576199.0737304688\n",
      "Epoch 1300 Loss: 576204.3315429688\n",
      "Epoch 1400 Loss: 576201.1235351562\n",
      "Epoch 1500 Loss: 576201.2734375\n",
      "Epoch 1600 Loss: 576203.302734375\n",
      "Epoch 1700 Loss: 576205.607421875\n",
      "Epoch 1800 Loss: 576196.8564453125\n",
      "Epoch 1900 Loss: 576197.5126953125\n",
      "Epoch 2000 Loss: 576200.8984375\n",
      "Epoch 2100 Loss: 576192.80078125\n",
      "Epoch 2200 Loss: 576209.6684570312\n",
      "Epoch 2300 Loss: 576211.6010742188\n",
      "Epoch 2400 Loss: 576208.1000976562\n",
      "Epoch 2500 Loss: 576209.5830078125\n",
      "Epoch 2600 Loss: 576203.9321289062\n",
      "Epoch 2700 Loss: 576198.787109375\n",
      "Epoch 2800 Loss: 576207.2626953125\n",
      "Epoch 2900 Loss: 576200.1313476562\n",
      "Epoch 3000 Loss: 576199.7895507812\n",
      "Epoch 3100 Loss: 576202.2592773438\n",
      "Epoch 3200 Loss: 576202.099609375\n",
      "Epoch 3300 Loss: 576202.7143554688\n",
      "Epoch 3400 Loss: 576206.6811523438\n",
      "Epoch 3500 Loss: 576212.3559570312\n",
      "Epoch 3600 Loss: 576201.0698242188\n",
      "Epoch 3700 Loss: 576204.6635742188\n",
      "Epoch 3800 Loss: 576201.7260742188\n",
      "Epoch 3900 Loss: 576194.8940429688\n",
      "Epoch 4000 Loss: 576204.4985351562\n",
      "Epoch 4100 Loss: 576200.9497070312\n",
      "Epoch 4200 Loss: 576199.9897460938\n",
      "Epoch 4300 Loss: 576201.2524414062\n",
      "Epoch 4400 Loss: 576198.0234375\n",
      "Epoch 4500 Loss: 576202.9482421875\n",
      "Epoch 4600 Loss: 576206.8959960938\n",
      "Epoch 4700 Loss: 576197.2490234375\n",
      "Epoch 4800 Loss: 576202.3896484375\n",
      "Epoch 4900 Loss: 576199.6015625\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "def model(features, p, labels=None):\n",
    "    n_samples, n_features = features.shape\n",
    "    \n",
    "    # If p_prior is a scalar, we replicate it to match the batch shape.\n",
    "    probs = p * torch.ones(n_samples, dtype=torch.float32)\n",
    "    \n",
    "    with pyro.plate('data', size=n_samples):\n",
    "        # capture samples of p for the posterior\n",
    "        pyro.sample('p_aux', dist.Delta(probs), obs=probs)\n",
    "        \n",
    "        if labels is not None:\n",
    "            y = pyro.sample('y', Bernoulli(probs), obs=labels.float())\n",
    "        else:\n",
    "            y = pyro.sample('y', Bernoulli(probs))\n",
    "        \n",
    "        for i in range(n_features):\n",
    "            discretized_feature = discretize(features[:, i])\n",
    "            pyro.sample(f'obs_{i}', Categorical(probs=torch.ones(12)/12), obs=discretized_feature)\n",
    "\n",
    "\n",
    "# make a nn guide class\n",
    "class NNGuide(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=10):\n",
    "        super(NNGuide, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, features):\n",
    "        x = torch.relu(self.fc1(features))\n",
    "        return torch.sigmoid(self.fc2(x))\n",
    "\n",
    "\n",
    "# instantiate nn guide\n",
    "guide_model = NNGuide(23)  # Assuming 23 features\n",
    "\n",
    "\n",
    "# def model guide\n",
    "def guide(features, p, labels=None):\n",
    "    estimated_p = guide_model(features).squeeze(-1)\n",
    "\n",
    "    with pyro.plate('data', size=features.shape[0]):\n",
    "        pyro.sample('y', dist.Bernoulli(estimated_p))\n",
    "        pyro.sample('p_aux', dist.Delta(estimated_p))\n",
    "\n",
    "# train model and guide using SVI\n",
    "optimizer = Adam({'lr': 0.01})\n",
    "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "\n",
    "# prior based on labels\n",
    "p_prior = float(train.y.sum() / len(train.y))\n",
    "\n",
    "labels_reshaped = labels.unsqueeze(-1)\n",
    "\n",
    "num_epochs = 5000 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss = svi.step(features.float(), p_prior, labels.long())\n",
    "    if epoch % 100 == 0:  # print loss every 100 steps\n",
    "        print(f'Epoch {epoch} Loss: {loss}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = torch.tensor(\n",
    "    test[feature_names].values,\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "test_labels = torch.tensor(\n",
    "    test.y.values,\n",
    "    dtype=torch.long\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the new data through the trained guide model\n",
    "with torch.no_grad():\n",
    "    p_posterior_mean = guide_model(test_features).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "\n",
    "predictive = Predictive(model, guide=guide, num_samples=num_samples, return_sites=['y', 'p_aux'])\n",
    "\n",
    "samples = predictive(test_features, p_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7693, 0.7693, 0.7693,  ..., 0.7693, 0.7693, 0.7693],\n",
       "        [0.7693, 0.7693, 0.7693,  ..., 0.7693, 0.7693, 0.7693],\n",
       "        [0.7693, 0.7693, 0.7693,  ..., 0.7693, 0.7693, 0.7693],\n",
       "        ...,\n",
       "        [0.7693, 0.7693, 0.7693,  ..., 0.7693, 0.7693, 0.7693],\n",
       "        [0.7693, 0.7693, 0.7693,  ..., 0.7693, 0.7693, 0.7693],\n",
       "        [0.7693, 0.7693, 0.7693,  ..., 0.7693, 0.7693, 0.7693]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples['p_aux']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'p_aux'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/michael/TreeMortality/src/naive_bayes_mortality.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/michael/TreeMortality/src/naive_bayes_mortality.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m y_samp \u001b[39m=\u001b[39m samples[\u001b[39m'\u001b[39;49m\u001b[39mp_aux\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/michael/TreeMortality/src/naive_bayes_mortality.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m y_samp\n",
      "\u001b[0;31mKeyError\u001b[0m: 'p_aux'"
     ]
    }
   ],
   "source": [
    "y_samp = samples['p_aux']\n",
    "y_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5310)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
