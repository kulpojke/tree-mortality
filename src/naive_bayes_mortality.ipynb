{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pyro\n",
    "from pyro.distributions import Categorical, Bernoulli\n",
    "from pyro import param\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import MCMC, NUTS\n",
    "from pyro.distributions import constraints\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import Predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_path = Path.cwd().parent / 'train_test_features.parquet'\n",
    "data = pd.read_parquet(feature_path) \n",
    "train, test = train_test_split(data, test_size=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(data):\n",
    "    '''\n",
    "    transforms continuous data in the range [0,1] and\n",
    "    no data values of  -99 to integers from 0 to 11.\n",
    "    Where 11 is no data value'''\n",
    "    # Handle the special case\n",
    "    special_case = (data == -99).int() * 11\n",
    "    discretized = (torch.clamp(torch.round(data * 10).int(), 0, 10) + special_case).long()\n",
    "    return discretized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ahh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features, found in mortality_classification.ipynb\n",
    "# based on abs(corr) < 0.4, and RF feature importance\n",
    "feature_names = [\n",
    "    'n_mean',\n",
    "    'lum70',\n",
    "    'savi_std',\n",
    "    'rgi_std',\n",
    "    'lum50',\n",
    "    'lum30',\n",
    "    'b_std',\n",
    "    'r_std',\n",
    "    'r40',\n",
    "    'lum40',\n",
    "    'g30',\n",
    "    'b50',\n",
    "    'lum10',\n",
    "    'b10',\n",
    "    'n10',\n",
    "    'rgi60',\n",
    "    'rgi80',\n",
    "    'r60',\n",
    "    'b80',\n",
    "    'rgi30',\n",
    "    'r70',\n",
    "    'n80',\n",
    "    'b60'\n",
    " ]\n",
    "\n",
    "features = torch.tensor(\n",
    "    train[feature_names].values,\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "labels = torch.tensor(\n",
    "    train.y.values,\n",
    "    dtype=torch.long\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/TreeMortality/venvs/pyro/lib/python3.10/site-packages/pyro/util.py:288: UserWarning: Found non-auxiliary vars in guide but not model, consider marking these infer={'is_auxiliary': True}:\n",
      "{'p_aux'}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 573973.5424804688\n",
      "Epoch 100 Loss: 573830.1557617188\n",
      "Epoch 200 Loss: 573947.1508789062\n",
      "Epoch 300 Loss: 573984.6938476562\n",
      "Epoch 400 Loss: 573915.4985351562\n",
      "Epoch 500 Loss: 574005.5385742188\n",
      "Epoch 600 Loss: 573904.9965820312\n",
      "Epoch 700 Loss: 573887.6791992188\n",
      "Epoch 800 Loss: 573870.2856445312\n",
      "Epoch 900 Loss: 573946.1811523438\n",
      "Epoch 1000 Loss: 573909.9057617188\n",
      "Epoch 1100 Loss: 573804.8002929688\n",
      "Epoch 1200 Loss: 573880.7661132812\n",
      "Epoch 1300 Loss: 574051.5874023438\n",
      "Epoch 1400 Loss: 574029.0844726562\n",
      "Epoch 1500 Loss: 574011.7563476562\n",
      "Epoch 1600 Loss: 573993.4711914062\n",
      "Epoch 1700 Loss: 573923.0805664062\n",
      "Epoch 1800 Loss: 573939.5063476562\n",
      "Epoch 1900 Loss: 573997.3471679688\n",
      "Epoch 2000 Loss: 573852.9780273438\n",
      "Epoch 2100 Loss: 573845.9204101562\n",
      "Epoch 2200 Loss: 573982.3559570312\n",
      "Epoch 2300 Loss: 573846.6596679688\n",
      "Epoch 2400 Loss: 574211.8256835938\n",
      "Epoch 2500 Loss: 573914.2729492188\n",
      "Epoch 2600 Loss: 573828.7202148438\n",
      "Epoch 2700 Loss: 573957.3637695312\n",
      "Epoch 2800 Loss: 574006.0805664062\n",
      "Epoch 2900 Loss: 573802.6440429688\n",
      "Epoch 3000 Loss: 573916.8793945312\n",
      "Epoch 3100 Loss: 573947.7416992188\n",
      "Epoch 3200 Loss: 573910.6918945312\n",
      "Epoch 3300 Loss: 574171.7036132812\n",
      "Epoch 3400 Loss: 573834.2368164062\n",
      "Epoch 3500 Loss: 574068.4194335938\n",
      "Epoch 3600 Loss: 573928.1508789062\n",
      "Epoch 3700 Loss: 573958.3129882812\n",
      "Epoch 3800 Loss: 573966.0200195312\n",
      "Epoch 3900 Loss: 573756.7133789062\n",
      "Epoch 4000 Loss: 573906.2973632812\n",
      "Epoch 4100 Loss: 573987.7963867188\n",
      "Epoch 4200 Loss: 573918.1118164062\n",
      "Epoch 4300 Loss: 573886.9497070312\n",
      "Epoch 4400 Loss: 573887.0668945312\n",
      "Epoch 4500 Loss: 574023.3959960938\n",
      "Epoch 4600 Loss: 573843.0200195312\n",
      "Epoch 4700 Loss: 573919.9926757812\n",
      "Epoch 4800 Loss: 573779.0727539062\n",
      "Epoch 4900 Loss: 573955.8686523438\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "def model(features, p, labels=None):\n",
    "    n_samples, n_features = features.shape\n",
    "    \n",
    "    # If p_prior is a scalar, we replicate it to match the batch shape.\n",
    "    probs = p * torch.ones(n_samples, dtype=torch.float32)\n",
    "    \n",
    "    with pyro.plate('data', size=n_samples):\n",
    "        # capture samples of p for the posterior\n",
    "        pyro.sample('p_aux', dist.Delta(probs), obs=probs)\n",
    "        \n",
    "        if labels is not None:\n",
    "            y = pyro.sample('y', Bernoulli(probs), obs=labels.float())\n",
    "        else:\n",
    "            y = pyro.sample('y', Bernoulli(probs))\n",
    "        \n",
    "        for i in range(n_features):\n",
    "            discretized_feature = discretize(features[:, i])\n",
    "            pyro.sample(f'obs_{i}', Categorical(probs=torch.ones(12)/12), obs=discretized_feature)\n",
    "\n",
    "\n",
    "# make a nn guide class\n",
    "class NNGuide(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=10):\n",
    "        super(NNGuide, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc_mu = nn.Linear(hidden_size, 1)  # Outputs the mean of the normal distribution\n",
    "        self.fc_sigma = nn.Linear(hidden_size, 1)  # Outputs the log standard deviation\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = torch.relu(self.fc1(features))\n",
    "        mu = self.fc_mu(x)\n",
    "        sigma = F.softplus(self.fc_sigma(x))  # Ensure sigma is positive\n",
    "        return mu, sigma\n",
    "\n",
    "\n",
    "# instantiate nn guide\n",
    "guide_model = NNGuide(features.shape[-1])\n",
    "\n",
    "\n",
    "# def model guide\n",
    "def guide(features, p, labels=None):\n",
    "    mu, sigma = guide_model(features)\n",
    "    mu = mu.squeeze(-1)\n",
    "    sigma = sigma.squeeze(-1)\n",
    "    \n",
    "    with pyro.plate('data', size=features.shape[0]):\n",
    "        pyro.sample('p_aux', dist.LogNormal(mu, sigma))\n",
    "    \n",
    "\n",
    "# train model and guide using SVI\n",
    "optimizer = Adam({'lr': 0.01})\n",
    "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "\n",
    "# prior based on labels\n",
    "p_prior = float(train.y.sum() / len(train.y))\n",
    "\n",
    "labels_reshaped = labels.unsqueeze(-1)\n",
    "\n",
    "num_epochs = 5000 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss = svi.step(features.float(), p_prior, labels.long())\n",
    "    if epoch % 100 == 0:  # print loss every 100 steps\n",
    "        print(f'Epoch {epoch} Loss: {loss}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = torch.tensor(\n",
    "    test[feature_names].values,\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "test_labels = torch.tensor(\n",
    "    test.y.values,\n",
    "    dtype=torch.long\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the new data through the trained guide model\n",
    "with torch.no_grad():\n",
    "    post_mu, post_sigma = guide_model(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0824],\n",
       "        [13.0301],\n",
       "        [-0.0310],\n",
       "        ...,\n",
       "        [-0.0480],\n",
       "        [-0.0272],\n",
       "        [-0.0365]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "\n",
    "predictive = Predictive(model, guide=guide, num_samples=num_samples, return_sites=['y', 'p_aux'])\n",
    "\n",
    "samples = predictive(test_features, p_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7693, 0.7693, 0.7693,  ..., 0.7693, 0.7693, 0.7693],\n",
       "        [0.7693, 0.7693, 0.7693,  ..., 0.7693, 0.7693, 0.7693],\n",
       "        [0.7693, 0.7693, 0.7693,  ..., 0.7693, 0.7693, 0.7693],\n",
       "        ...,\n",
       "        [0.7693, 0.7693, 0.7693,  ..., 0.7693, 0.7693, 0.7693],\n",
       "        [0.7693, 0.7693, 0.7693,  ..., 0.7693, 0.7693, 0.7693],\n",
       "        [0.7693, 0.7693, 0.7693,  ..., 0.7693, 0.7693, 0.7693]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples['p_aux']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'p_aux'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/michael/TreeMortality/src/naive_bayes_mortality.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/michael/TreeMortality/src/naive_bayes_mortality.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m y_samp \u001b[39m=\u001b[39m samples[\u001b[39m'\u001b[39;49m\u001b[39mp_aux\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/michael/TreeMortality/src/naive_bayes_mortality.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m y_samp\n",
      "\u001b[0;31mKeyError\u001b[0m: 'p_aux'"
     ]
    }
   ],
   "source": [
    "y_samp = samples['p_aux']\n",
    "y_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5310)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
