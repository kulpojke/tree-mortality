{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "from statistics import mode\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import rioxarray\n",
    "\n",
    "from xrspatial import focal, slope\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from joblib_progress import joblib_progress\n",
    "from xrspatial.multispectral import ndvi, savi\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (confusion_matrix, ConfusionMatrixDisplay)\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV as RSCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, log_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "helena_path = Path.cwd().parent / 'data' / 'helena'\n",
    "feature_dir = helena_path / 'features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['features_2018_crowns_100.parquet']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get paths for features using 100 m geomorphons\n",
    "parquets = [p for p in os.listdir(feature_dir) if p.endswith('_100.parquet')]\n",
    "parquets.sort()\n",
    "\n",
    "parquets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the model which was tuned and trained in `src/mortality_classification.ipynb`.  It was pickled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model created in src/mortality_classification.ipynb\n",
    "pickle_path = Path.cwd() / 'RF_model.sav'\n",
    "model = pickle.load(open(pickle_path, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make model predictions for the samples and create a timeseries of survival probabilities for each sample over the years for which we have NAIP data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------features_2018_crowns_100.parquet----------\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     y[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpred_\u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict_proba(X)[:, \u001b[39m1\u001b[39m]\n\u001b[1;32m     17\u001b[0m     predictions\u001b[39m.\u001b[39mappend(y)\n\u001b[0;32m---> 19\u001b[0m predictions \u001b[39m=\u001b[39m [predictions[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mjoin(df_, on\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mUniqueID\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mfor\u001b[39;49;00m df_ \u001b[39min\u001b[39;49;00m predictions[\u001b[39m1\u001b[39;49m:]][\u001b[39m0\u001b[39;49m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for f in parquets:\n",
    "    print(f'-------{f}----------')\n",
    "    \n",
    "    # get year and geomorphon radius\n",
    "    split_fname = f.split('_')\n",
    "    y = split_fname[1]\n",
    "    r = split_fname[3].split('.')[0]\n",
    "    \n",
    "    # read parquet, make input feature df (X)\n",
    "    df = pd.read_parquet(feature_dir / f)\n",
    "    cols = list(model.feature_names_in_)\n",
    "    X = df[cols]\n",
    "    y = pd.DataFrame()\n",
    "    y['UniqueID'] = df['UniqueID']\n",
    "    y[f'pred_{y}'] = model.predict_proba(X)[:, 1]\n",
    "    predictions.append(y)\n",
    "    \n",
    "predictions = [predictions[0].join(df_, on='UniqueID') for df_ in predictions[1:]][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n_mean        0\n",
       "rgi_mean      0\n",
       "r_mean        0\n",
       "n50           0\n",
       "r30           0\n",
       "rgi40         0\n",
       "r20           0\n",
       "n30           0\n",
       "rgi50         0\n",
       "n60           0\n",
       "b20           0\n",
       "g_mean        0\n",
       "b30           0\n",
       "g40           0\n",
       "savi_std     93\n",
       "ndvi_mean    93\n",
       "savi_mean    93\n",
       "b_std         0\n",
       "r10           0\n",
       "b_mean        0\n",
       "lum70         0\n",
       "rgi_std       0\n",
       "n_std         0\n",
       "lum30         0\n",
       "n20           0\n",
       "lum50         0\n",
       "g20           0\n",
       "r_std         0\n",
       "lum20         0\n",
       "lum60         0\n",
       "g30           0\n",
       "lum40         0\n",
       "ndvi_std     93\n",
       "n40           0\n",
       "b40           0\n",
       "g_std         0\n",
       "r40           0\n",
       "b50           0\n",
       "lum10         0\n",
       "lum80         0\n",
       "n70           0\n",
       "b10           0\n",
       "r70           0\n",
       "rgi60         0\n",
       "g100          0\n",
       "n10           0\n",
       "rgi90         0\n",
       "g90           0\n",
       "n100          0\n",
       "lum90         0\n",
       "r50           0\n",
       "b60           0\n",
       "g50           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(feature_dir / f)\n",
    "cols = list(model.feature_names_in_)\n",
    "X = df[cols]\n",
    "X.isin([np.inf, -np.inf]).sum()\n",
    "\n",
    "#X.replace([np.inf, -np.inf], np.nan).dropna()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "treemort",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
