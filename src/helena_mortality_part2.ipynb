{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "from statistics import mode\n",
    "\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import rioxarray\n",
    "import cupy\n",
    "\n",
    "from xrspatial import hillshade\n",
    "from xrspatial import convolution\n",
    "from datashader.colors import Set1\n",
    "from datashader.transfer_functions import shade\n",
    "from datashader.transfer_functions import stack\n",
    "from datashader.transfer_functions import dynspread\n",
    "from datashader.transfer_functions import set_background\n",
    "from datashader.colors import Elevation\n",
    "\n",
    "from xrspatial import focal, slope\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from joblib_progress import joblib_progress\n",
    "from xrspatial.multispectral import ndvi, savi\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (confusion_matrix, ConfusionMatrixDisplay)\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV as RSCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, log_loss\n",
    "\n",
    "# paths\n",
    "high_high_path = '/home/michael/TreeMortality/data/helena/treatment_polys/code15_n5.gpkg'\n",
    "high_un_path = '/home/michael/TreeMortality/data/helena/treatment_polys/code12_n5.gpkg'\n",
    "un_high_path = '/home/michael/TreeMortality/data/helena/treatment_polys/code3_n5.gpkg'\n",
    "un_un_path = '/home/michael/TreeMortality/data/helena/treatment_polys/code0_n5.gpkg'\n",
    "poly_paths = [high_high_path, high_un_path, un_high_path, un_un_path]\n",
    "\n",
    "\n",
    "helena_path = Path.cwd().parent / 'data' / 'helena'\n",
    "spectral_crowns_path = helena_path / 'spectral_crowns'\n",
    "geomorph_dir = helena_path / 'geomorphons'\n",
    "crown_path = helena_path / 'crowns'\n",
    "crown_path_list = [\n",
    "    c for c\n",
    "    in crown_path.iterdir()\n",
    "    if c.suffix == '.gpkg'\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to engineer the features for our model, as done in `src/mortality_classification.ipynb`.  Because the Naip imagery for the area is so large, in order for this to run on my machine, the dtype of the imagery has been changed to `np.float32`.  This should not make a difference in the results, but is worth noting.  in `mortality_classification_geographic_holdouts.ipynb` feature creation was run in parallel, but tha tis not possible with sucha large image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_inputs(crowns, tif, save_path, y, gk, label=None, IDcolumn=None):\n",
    "    '''\n",
    "    Returns DataFrame with features for use in classification model.\n",
    "    The resulting DataFrame has 'ID' column which matches that in crowns.\n",
    "    The DataFrame also has a 'label' column, see params for more detail.  \n",
    "\n",
    "    params:\n",
    "        crowns   - str - path to OGR readable vector file containing tree crowns.\n",
    "        tif      - xr  - image used in producing features, already read\n",
    "                         with rioxarray.\n",
    "        label    - str - specifies column containing labels.  If specified 'label'\n",
    "                         column in resulting DataFrame will contain contents of \n",
    "                         specified column. Otherwise 'label' column contain -99.\n",
    "        IDcolumn - str - column to use as matching ID with crowns\n",
    "    '''\n",
    "    try:\n",
    "        # get the extent of the crowns\n",
    "        xmin, ymin, xmax, ymax = crowns.total_bounds\n",
    "\n",
    "        # clip the naip image\n",
    "        print(f'\\t\\t--clipping...')\n",
    "        xa = tif.astype(np.float32).rio.clip_box(\n",
    "            minx=xmin,\n",
    "            miny=ymin,\n",
    "            maxx=xmax,\n",
    "            maxy=ymax\n",
    "            ).to_dataset(name='band_data')\n",
    "\n",
    "        # normalized the band_data\n",
    "        print(f'\\t\\t--normalizing...')\n",
    "        band_data = xa.band_data.to_numpy().astype(np.float32)\n",
    "        band_data = (band_data - np.nanmin(band_data)) * (255 / (np.nanmax(band_data) - np.nanmin(band_data)))\n",
    "\n",
    "        # calculate relative greenness\n",
    "        print(f'\\t\\t--calculating RGI...')\n",
    "        red = band_data[0]\n",
    "        green = band_data[1]\n",
    "        blue = band_data[2]\n",
    "        nir = band_data[3]\n",
    "        rgi = green / (red + green + blue)\n",
    "        xa['rgi'] = (('y', 'x'), rgi)\n",
    "\n",
    "        # calculate pixel by pixel normalized R, G, B, and NIR\n",
    "        print(f'\\t\\t--pix norming...')\n",
    "        rgbn_tot = red + green + blue + nir\n",
    "        xa['red_'] = (('y', 'x'), red  / rgbn_tot)\n",
    "        xa['blue_'] = (('y', 'x'), blue  / rgbn_tot)\n",
    "        xa['green_'] = (('y', 'x'), green  / rgbn_tot)\n",
    "        xa['nir_'] = (('y', 'x'), nir  / rgbn_tot)\n",
    "\n",
    "        # calculate NDVI and SAVI\n",
    "        print(f'\\t\\t--NDVI and SAVI...')\n",
    "        nir_agg = xa.band_data[3].astype(np.float32)\n",
    "        red_agg = xa.band_data[2].astype(np.float32)\n",
    "        ndvi_agg = ndvi(nir_agg, red_agg)\n",
    "        savi_agg = savi(nir_agg, red_agg)\n",
    "        xa['NDVI'] = ndvi_agg\n",
    "        xa['SAVI'] = savi_agg\n",
    "\n",
    "        # calculate RGB luminosity\n",
    "        print(f'\\t\\t--luminosity...')\n",
    "        luminosity = band_data[:3].mean(axis=0) / 255\n",
    "        xa['luminosity'] = (('y', 'x'), luminosity)\n",
    "\n",
    "        # mask out shadows and soil for RGI,NDVI, and normed pix colors\n",
    "        print(f'\\t\\t--masking...')\n",
    "        mask = (luminosity > 0.176) & (luminosity < 0.569)\n",
    "        masked_rgi = xa.rgi.where(mask)\n",
    "        masked_ndvi = xa.NDVI.where(mask)\n",
    "        r_ = xa.red_.where(mask)\n",
    "        g_ = xa.green_.where(mask)\n",
    "        b_ = xa.blue_.where(mask)\n",
    "        n_ = xa.nir_.where(mask)\n",
    "        \n",
    "        print(f'\\t\\t--adding index data...')\n",
    "        data = []\n",
    "        masked_count = 0\n",
    "        total = len(crowns)\n",
    "        bins = np.arange(0.1, 1.1, 0.1)\n",
    "        with tqdm(total=total) as progress_bar:\n",
    "            for _, row in crowns.iterrows():\n",
    "                # calculate luminosity fractions\n",
    "                lum = xa.luminosity.rio.clip([row.geometry]).to_numpy().flatten()\n",
    "                lum_tot = lum.shape[0]\n",
    "                lum_fracs = [((lum < f).sum() - (lum < f - 0.1).sum()) / lum_tot for f in bins]\n",
    "\n",
    "                # calculate rgi fracs\n",
    "                rgi = masked_rgi.rio.clip([row.geometry]).to_numpy().flatten()\n",
    "                rgi = rgi[~np.isnan(rgi)]\n",
    "                rgi_tot = len(rgi)\n",
    "                if rgi_tot == 0:\n",
    "                    rgi_fracs = [-99] * 10\n",
    "                else:\n",
    "                    rgi_fracs = [((rgi < f).sum() - (rgi < f - 0.1).sum()) / rgi_tot for f in bins]\n",
    "                    \n",
    "                # and normed pix colr fracs\n",
    "                r = r_.rio.clip([row.geometry]).to_numpy().flatten()\n",
    "                r = r[~np.isnan(r)]\n",
    "                c_tot = len(r)\n",
    "                \n",
    "                g = g_.rio.clip([row.geometry]).to_numpy().flatten()\n",
    "                g = g[~np.isnan(g)]\n",
    "\n",
    "                b = b_.rio.clip([row.geometry]).to_numpy().flatten()\n",
    "                b = b[~np.isnan(b)]\n",
    "\n",
    "                n = n_.rio.clip([row.geometry]).to_numpy().flatten()\n",
    "                n = n[~np.isnan(n)]\n",
    "\n",
    "                if c_tot == 0:\n",
    "                    r_fracs = [-99] * 10\n",
    "                    g_fracs = [-99] * 10\n",
    "                    b_fracs = [-99] * 10\n",
    "                    n_fracs = [-99] * 10\n",
    "                else:\n",
    "                    r_fracs = [((r < f).sum() - (r < f - 0.1).sum()) / c_tot for f in bins]\n",
    "                    g_fracs = [((g < f).sum() - (g < f - 0.1).sum()) / c_tot for f in bins]\n",
    "                    b_fracs = [((b < f).sum() - (b < f - 0.1).sum()) / c_tot for f in bins]\n",
    "                    n_fracs = [((n < f).sum() - (n < f - 0.1).sum()) / c_tot for f in bins]\n",
    "                            \n",
    "                # calculate means and stdevs\n",
    "                if rgi_tot == 0:\n",
    "                    ndvi_mean, ndvi_std = -99, -99\n",
    "                    rgi_mean, rgi_std = -99, -99\n",
    "                    savi_mean, savi_std = -99, -99\n",
    "                    r_mean, r_std = -99, -99\n",
    "                    g_mean, g_std = -99, -99\n",
    "                    b_mean, b_std = -99, -99\n",
    "                    n_mean, n_std = -99, -99\n",
    "                else:\n",
    "                    #NOTE: .values * 1 casts 1 item DataArray to float\n",
    "                    ndvi_mean, ndvi_std = masked_ndvi.mean().values * 1, masked_ndvi.std().values * 1\n",
    "                    rgi_mean, rgi_std = rgi.mean(), rgi.std()\n",
    "                    savi_mean, savi_std = xa.SAVI.mean().values * 1, xa.SAVI.std().values * 1\n",
    "                    r_mean, r_std = r.mean(), r.std()\n",
    "                    g_mean, g_std = g.mean(), g.std()\n",
    "                    b_mean, b_std = b.mean(), b.std()\n",
    "                    n_mean, n_std = n.mean(), n.std()\n",
    "\n",
    "                if label is None:\n",
    "                    row[label] = -99\n",
    "\n",
    "                data.append(\n",
    "                    [row[IDcolumn], (row[label] + 1) / 2] +\n",
    "                    lum_fracs +\n",
    "                    rgi_fracs + \n",
    "                    r_fracs + \n",
    "                    g_fracs + \n",
    "                    b_fracs + \n",
    "                    n_fracs +\n",
    "                    [ndvi_mean, ndvi_std, rgi_mean, rgi_std, savi_mean, savi_std] +\n",
    "                    [r_mean, r_std, g_mean, g_std, b_mean, b_std, n_mean, n_std]\n",
    "                    )\n",
    "\n",
    "                #count polygon if has masked pixels            \n",
    "                if rgi_tot < len(xa.rgi.rio.clip([row.geometry]).to_numpy().flatten()):\n",
    "                    masked_count = masked_count + 1\n",
    "\n",
    "                progress_bar.update(1)\n",
    "\n",
    "        cols = [IDcolumn, 'label',\n",
    "                'lum10', 'lum20', 'lum30', 'lum40', 'lum50', 'lum60' ,'lum70', 'lum80', 'lum90', 'lum100',\n",
    "                'rgi10', 'rgi20', 'rgi30', 'rgi40', 'rgi50', 'rgi60' ,'rgi70', 'rgi80', 'rgi90', 'rgi100',\n",
    "                'r10', 'r20', 'r30', 'r40', 'r50', 'r60' ,'r70', 'r80', 'r90', 'r100',\n",
    "                'g10', 'g20', 'g30', 'g40', 'g50', 'g60' ,'g70', 'g80', 'g90', 'g100',\n",
    "                'b10', 'b20', 'b30', 'b40', 'b50', 'b60' ,'b70', 'b80', 'b90', 'b100',\n",
    "                'n10', 'n20', 'n30', 'n40', 'n50', 'n60' ,'n70', 'n80', 'n90', 'n100',\n",
    "                'ndvi_mean', 'ndvi_std', 'rgi_mean', 'rgi_std', 'savi_mean', 'savi_std',\n",
    "                'r_mean', 'r_std', 'g_mean', 'g_std', 'b_mean', 'b_std', 'n_mean', 'n_std']\n",
    "\n",
    "        data = pd.DataFrame(data, columns=cols)\n",
    "        data.to_parquet(save_path / f'features_{y}_{gk}.parquet')\n",
    "        print(y, gk, 'saved to ', str(helena_path / f'features_{y}_{gk}.parquet'))\n",
    "        del data\n",
    "    except:\n",
    "        print(f'OH NO!!! {y}, {gk} FAILED!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naip_dir = helena_path / 'NAIP'\n",
    "save_path = helena_path / 'features'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "def treatment_keys(gk):\n",
    "        if gk == 250:\n",
    "            return [0, 3, 12, 15]\n",
    "        return [3, 12, 15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t--clipping...\n",
      "\t\t--normalizing...\n",
      "\t\t--calculating RGI...\n",
      "\t\t--pix norming...\n",
      "\t\t--NDVI and SAVI...\n",
      "\t\t--luminosity...\n",
      "\t\t--masking...\n",
      "\t\t--adding index data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/223 [01:06<4:05:18, 66.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OH NO!!! 2018, geomorph_250 FAILED!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "y = 2018\n",
    "gk = 'geomorph_250'\n",
    "tif_path = naip_dir / str(y) / f'{y}.vrt'\n",
    "xa = rioxarray.open_rasterio(tif_path)\n",
    "                 \n",
    "make_model_inputs(\n",
    "    samples[gk],\n",
    "    xa,\n",
    "    save_path, y, gk,\n",
    "    label=None,\n",
    "    IDcolumn='UniqueID'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = {}\n",
    "\n",
    "for y in [2018, 2020, 2022]:\n",
    "    print(f'----------{y}-----------')                   \n",
    "    tif_path = naip_dir / str(y) / f'{y}.vrt'\n",
    "    xa = rioxarray.open_rasterio(tif_path)\n",
    "    features[y] = {}\n",
    "    for gk in samples.keys():\n",
    "        print(f'\\t------{gk}/{len(samples.keys())}--------')                   \n",
    "        features[y][gk] = make_model_inputs(\n",
    "            samples[gk],\n",
    "            xa,\n",
    "            save_path, y, gk,\n",
    "            label=None,\n",
    "            IDcolumn='UniqueID'\n",
    "            )\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the model which was tuned and trained in `src/mortality_classification.ipynb`.  It was pickled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model created in src/mortality_classification.ipynb\n",
    "pickle_path = Path.cwd() / 'RF_model.sav'\n",
    "model = pickle.load(open(pickle_path, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make model predictions for the samples and create a timeseries of survival probabilities for each sample over the years for which we have NAIP data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_keys = dict_of_samples.keys()\n",
    "geomorphon_keys = range(1,11)\n",
    "years = [2018, 2020, 2022]\n",
    "\n",
    "for tk in treatment_keys:\n",
    "    for gk in geomorphon_keys:\n",
    "        ...\n",
    "def sample_mortality_timeseries(sample_dict, years):\n",
    "    '''\n",
    "    Takes a dict of years for a given sample,\n",
    "    returns a df of probabilities of being alive\n",
    "    by year.\n",
    "    '''\n",
    "    t_series = []\n",
    "    for y in years:\n",
    "        cols = sample_dict[y].drop(['y', 'label', 'UniqueID'], axis=1).columns\n",
    "        X = sample_dict[y][cols]\n",
    "        lil_df = pd.DataFrame()\n",
    "        lil_df['UniqueID'] = sample_dict[y]['UniqueID']\n",
    "        lil_df['pred'] = model.predict_proba(X)[:, 1]\n",
    "        t_series.append(lil_df)\n",
    "        \n",
    "    t_series = [t_series[0].join(df_, on='UniqueID') for df_ in t_series[1:]][0]\n",
    "        \n",
    "    return t_series\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
