{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "from statistics import mode\n",
    "\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import rioxarray\n",
    "import cupy\n",
    "\n",
    "from xrspatial import hillshade\n",
    "from xrspatial import convolution\n",
    "from datashader.colors import Set1\n",
    "from datashader.transfer_functions import shade\n",
    "from datashader.transfer_functions import stack\n",
    "from datashader.transfer_functions import dynspread\n",
    "from datashader.transfer_functions import set_background\n",
    "from datashader.colors import Elevation\n",
    "\n",
    "from xrspatial import focal, slope\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from joblib_progress import joblib_progress\n",
    "from xrspatial.multispectral import ndvi, savi\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (confusion_matrix, ConfusionMatrixDisplay)\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV as RSCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "high_high_path = '/home/michael/TreeMortality/data/helena/treatment_polys/code15_n5.gpkg'\n",
    "high_un_path = '/home/michael/TreeMortality/data/helena/treatment_polys/code12_n5.gpkg'\n",
    "un_high_path = '/home/michael/TreeMortality/data/helena/treatment_polys/code3_n5.gpkg'\n",
    "un_un_path = '/home/michael/TreeMortality/data/helena/treatment_polys/code0_n5.gpkg'\n",
    "poly_paths = [high_high_path, high_un_path, un_high_path, un_un_path]\n",
    "\n",
    "helena_path = Path.cwd().parent / 'data' / 'helena'\n",
    "crown_path = helena_path / 'crowns'\n",
    "crown_path_list = [\n",
    "    c for c\n",
    "    in crown_path.iterdir()\n",
    "    if c.suffix == '.gpkg'\n",
    "    ]\n",
    "\n",
    "# open treatment polygons\n",
    "df = pd.concat([gpd.read_file(p) for p in poly_paths])\n",
    "df = df.drop('area_', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first check each crown to see if it falls completely within one of the treatment class areas.  If so it will be appended to a datframe of crowns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jobs to run in ||\n",
    "n_jobs = 23\n",
    "\n",
    "def is_in_treatment(crown_df, row, buf):\n",
    "    '''Returns only crowns with buffer completely within polygon'''\n",
    "    crown_df.loc[\n",
    "        buf.within(row.geometry),\n",
    "        'treatment'] = row.attribute\n",
    "    \n",
    "    return crown_df[crown_df.treatment >= 0 ]\n",
    "\n",
    "\n",
    "def label_treatment(f):\n",
    "    crown_df = gpd.read_file(f)\n",
    "    crown_df = crown_df[crown_df.geometry.area > 10]\n",
    "    \n",
    "    # get total bounds of tile as polygon\n",
    "    bounds = crown_df.total_bounds\n",
    "    bbox = shapely.geometry.box(*bounds)\n",
    "\n",
    "    # use only treatment geometries which touch the tile\n",
    "    sub_df = df[df.geometry.intersects(bbox)]\n",
    "    if len(sub_df) > 0:\n",
    "        # add treatment column\n",
    "        crown_df['treatment'] = -99\n",
    "        #buffer crowns\n",
    "        buf = crown_df.geometry.buffer(10)\n",
    "        # label treatments of crowns lying completely within poly\n",
    "        return Parallel(n_jobs=n_jobs)(\n",
    "            delayed(is_in_treatment)(crown_df, row, buf)\n",
    "            for _, row in sub_df.iterrows()\n",
    "            )\n",
    "    else:\n",
    "        # return empty df, but add treatment column first\n",
    "        cols = list(crown_df.columns) + ['treatment']\n",
    "        empty_df = pd.DataFrame(columns=cols)\n",
    "        return [empty_df]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f996a7f9d7214fc8a74c6b061bb5002b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with joblib_progress('', total=len(crown_path_list)):\n",
    "    results =  Parallel(n_jobs=n_jobs)(delayed(label_treatment)(f) for f in crown_path_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results is a list of lists of dfs, so we must flatten to concat\n",
    "crown_df = pd.concat([item for sublist in results for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "results = [label_treatment(f) for f in tqdm(crown_path_list)]\n",
    "crown_df = pd.concat(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we will also add a unique identifier.  Then save `crowns_df` so in case ware interrupted,  we will be able to resume without running the 5 hour block of code above again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDdalponte</th>\n",
       "      <th>zmax</th>\n",
       "      <th>zmean</th>\n",
       "      <th>zsd</th>\n",
       "      <th>zskew</th>\n",
       "      <th>zkurt</th>\n",
       "      <th>zentropy</th>\n",
       "      <th>pzabovezmean</th>\n",
       "      <th>pzabove2</th>\n",
       "      <th>zq5</th>\n",
       "      <th>...</th>\n",
       "      <th>p2th</th>\n",
       "      <th>p3th</th>\n",
       "      <th>p4th</th>\n",
       "      <th>p5th</th>\n",
       "      <th>pground</th>\n",
       "      <th>n</th>\n",
       "      <th>area</th>\n",
       "      <th>geometry</th>\n",
       "      <th>treatment</th>\n",
       "      <th>UniqueID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.29</td>\n",
       "      <td>4.970000</td>\n",
       "      <td>0.549181</td>\n",
       "      <td>-1.123796</td>\n",
       "      <td>2.307702</td>\n",
       "      <td>0.313845</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.3030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0672</td>\n",
       "      <td>POLYGON ((496566.730 4511249.660, 496566.620 4...</td>\n",
       "      <td>12</td>\n",
       "      <td>10N_496567_4511250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.93</td>\n",
       "      <td>5.684000</td>\n",
       "      <td>0.387337</td>\n",
       "      <td>-1.405252</td>\n",
       "      <td>3.124655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>5.1560</td>\n",
       "      <td>...</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0760</td>\n",
       "      <td>POLYGON ((496570.930 4511249.740, 496570.640 4...</td>\n",
       "      <td>12</td>\n",
       "      <td>10N_496571_4511250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>9.26</td>\n",
       "      <td>7.120000</td>\n",
       "      <td>2.037727</td>\n",
       "      <td>-0.419361</td>\n",
       "      <td>1.899452</td>\n",
       "      <td>0.602060</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.8005</td>\n",
       "      <td>...</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0638</td>\n",
       "      <td>POLYGON ((496589.250 4511249.710, 496589.000 4...</td>\n",
       "      <td>12</td>\n",
       "      <td>10N_496589_4511250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>13.39</td>\n",
       "      <td>11.101111</td>\n",
       "      <td>1.725648</td>\n",
       "      <td>0.357462</td>\n",
       "      <td>1.321602</td>\n",
       "      <td>0.435405</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>100.0</td>\n",
       "      <td>9.4440</td>\n",
       "      <td>...</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.1551</td>\n",
       "      <td>POLYGON ((496743.460 4511249.750, 496743.330 4...</td>\n",
       "      <td>12</td>\n",
       "      <td>10N_496743_4511250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.0</td>\n",
       "      <td>9.49</td>\n",
       "      <td>6.605000</td>\n",
       "      <td>2.987673</td>\n",
       "      <td>-0.013348</td>\n",
       "      <td>1.025824</td>\n",
       "      <td>0.439247</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.6300</td>\n",
       "      <td>...</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.1148</td>\n",
       "      <td>POLYGON ((496775.400 4511249.590, 496775.330 4...</td>\n",
       "      <td>12</td>\n",
       "      <td>10N_496775_4511250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   IDdalponte   zmax      zmean       zsd     zskew     zkurt  zentropy  \\\n",
       "0         2.0   5.29   4.970000  0.549181 -1.123796  2.307702  0.313845   \n",
       "1         3.0   5.93   5.684000  0.387337 -1.405252  3.124655  0.000000   \n",
       "2         4.0   9.26   7.120000  2.037727 -0.419361  1.899452  0.602060   \n",
       "3        10.0  13.39  11.101111  1.725648  0.357462  1.321602  0.435405   \n",
       "4        11.0   9.49   6.605000  2.987673 -0.013348  1.025824  0.439247   \n",
       "\n",
       "   pzabovezmean  pzabove2     zq5  ...       p2th       p3th  p4th  p5th  \\\n",
       "0     75.000000     100.0  4.3030  ...   0.000000   0.000000   0.0   0.0   \n",
       "1     80.000000     100.0  5.1560  ...  20.000000   0.000000   0.0   0.0   \n",
       "2     50.000000     100.0  4.8005  ...  25.000000   0.000000   0.0   0.0   \n",
       "3     44.444444     100.0  9.4440  ...  44.444444   0.000000   0.0   0.0   \n",
       "4     50.000000     100.0  3.6300  ...  50.000000  16.666667   0.0   0.0   \n",
       "\n",
       "   pground  n    area                                           geometry  \\\n",
       "0      0.0  4  0.0672  POLYGON ((496566.730 4511249.660, 496566.620 4...   \n",
       "1      0.0  5  0.0760  POLYGON ((496570.930 4511249.740, 496570.640 4...   \n",
       "2      0.0  4  0.0638  POLYGON ((496589.250 4511249.710, 496589.000 4...   \n",
       "3      0.0  9  0.1551  POLYGON ((496743.460 4511249.750, 496743.330 4...   \n",
       "4      0.0  6  0.1148  POLYGON ((496775.400 4511249.590, 496775.330 4...   \n",
       "\n",
       "   treatment            UniqueID  \n",
       "0         12  10N_496567_4511250  \n",
       "1         12  10N_496571_4511250  \n",
       "2         12  10N_496589_4511250  \n",
       "3         12  10N_496743_4511250  \n",
       "4         12  10N_496775_4511250  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_unique_ID(crowns, utm_zone):\n",
    "    '''\n",
    "    returns copy of dataframe with new uniqueID column\n",
    "    with entries of form 'utm_zone_x_y where x and y \n",
    "    are rounded to the nearest meter.\n",
    "    TODO: make it round to nearest even meter to lower precision\n",
    "    '''\n",
    "    crowns['UniqueID'] = crowns.geometry.centroid.apply(\n",
    "        lambda p: f'{utm_zone}_{p.x:.0f}_{p.y:.0f}')\n",
    "    \n",
    "    return crowns\n",
    "\n",
    "# add unique ID\n",
    "crown_df_ = make_unique_ID(crown_df, '10N')\n",
    "crown_df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "crown_df_.to_file(helena_path / 'crowns_with_treatment_label.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "treatment\n",
       "0     2794929\n",
       "3      601844\n",
       "12     468077\n",
       "15      11980\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure the number of treatments is reasonable\n",
    "crown_df_.treatment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geomorphons\n",
    "In order to look at the effects of slope position on tree mortality we will use the geomorphons algorithm  as implemented in Whitebox Tools.  We will use the geoporphons rasters that were calculated in `src/helena_geomorphon.ipynb`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now finally we know it is safe to make crown_df = crown_df_ \n",
    "#crown_df = crown_df_\n",
    "\n",
    "# or load it from file if you were interrupted\n",
    "crown_df = gpd.read_file(helena_path / 'crowns_with_treatment_label.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to rasters\n",
    "geomorph_dir = helena_path / 'geomorphons'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OMG, I forgot to do this at the begining, probably wasted a lot of time on tiny polys\n",
    "# fixed above for next time\n",
    "crown_df = crown_df[crown_df.geometry.area > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "treatment\n",
       "0     2666168\n",
       "3      527050\n",
       "12     415553\n",
       "15       6409\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crown_df.treatment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "n_jobs = 4\n",
    "\n",
    "def inner_func(gmorph, row):\n",
    "    zult = mode(gmorph.rio.clip([row.geometry]).to_numpy().flatten())\n",
    "    return (row.UniqueID, zult)\n",
    "\n",
    "results = []\n",
    "\n",
    "for r in tqdm([100, 250, 500, 1000, 2000]):\n",
    "    # add column full of no data vals\n",
    "    crown_df[f'geomorphon_{r}'] = -999\n",
    "    \n",
    "    # open geomorphon tif\n",
    "    tif = geomorph_dir / f'geomorph_{r}.tif' \n",
    "    gmorph = rioxarray.open_rasterio(tif)\n",
    "    \n",
    "    # attach landform to crowns\n",
    "    results.append(\n",
    "        pd.DataFrame(\n",
    "            Parallel(n_jobs=n_jobs)(\n",
    "                delayed(inner_func)(gmorph, row)\n",
    "                for _, row in crown_df.iterrows()\n",
    "            ),\n",
    "            columns=['UniqueID', f'geomorph_{r}']\n",
    "        )\n",
    "    )\n",
    "\n",
    "# join all results on UniqueID\n",
    "for data_frame_thing in results:\n",
    "    crown_df = crown_df.join(\n",
    "        data_frame_thing,\n",
    "        on='UniqueID'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a two-level stratified sample of crowns based on treatment and TPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict to hold samples\n",
    "dict_of_samples = {}\n",
    "\n",
    "# split into groups based on treatment\n",
    "for tr in [0, 3, 12, 15]:\n",
    "    df_x = crown_df[crown_df.treatment == tr]\n",
    "\n",
    "    # for each treatment, split based on slope position.\n",
    "    sub_dict = {}\n",
    "    for pos in range(1,11):\n",
    "        sub_dict[f'geomorph_{r}'] = df_x[df_x[f'geomorph_{r}'] == pos]\n",
    "    dict_of_samples[f'treatment_{tr}'] = sub_dict\n",
    "\n",
    "# find the size of the smallest smallest treatment/landform population\n",
    "n = np.inf\n",
    "for key1, val_dict in dict_of_samples.items():\n",
    "    n = min(n, min([len(d) for d in val_dict[key1]]))\n",
    "    \n",
    "print(f'The smallest treatment/landform population is {n}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get largest possible even sample for each treatment\n",
    "for key1, val_dict in dict_of_samples.items():\n",
    "    # get smallest  TPI population for given treatment\n",
    "    n = min([len(d) for d in val_dict[key1]])\n",
    "    # now sample each to that size\n",
    "    for key2, val in val_dict.items():\n",
    "        dict_of_samples[key1][key2] = dict_of_samples[key1][key2].sample(n=n)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to engineer the features for our model, as done in `src/mortality_classification_geographic_holdouts.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_inputs(crowns, tif_path, label=None, IDcolumn=None):\n",
    "    '''\n",
    "    Returns DataFrame with features for use in classification model.\n",
    "    The resulting DataFrame has 'ID' column which matches that in crowns.\n",
    "    The DataFrame also has a 'label' column, see params for more detail.  \n",
    "\n",
    "    params:\n",
    "        crowns   - str - path to OGR readable vector file containing tree crowns.\n",
    "        tif_path - str - path to image tif used in producing features.\n",
    "        label    - str - specifies column containing labels.  If specified 'label'\n",
    "                         column in resulting DataFrame will contain contents of \n",
    "                         specified column. Otherwise 'label' column will contain -99.\n",
    "        IDcolumn - str - column to use as matching ID with crowns\n",
    "    '''\n",
    "\n",
    "    # get the extent of the crowns\n",
    "    xmin, ymin, xmax, ymax = crowns.total_bounds\n",
    "\n",
    "    # open the naip image\n",
    "    xa = rioxarray.open_rasterio(tif_path).astype(float).rio.clip_box(\n",
    "        minx=xmin,\n",
    "        miny=ymin,\n",
    "        maxx=xmax,\n",
    "        maxy=ymax\n",
    "        ).to_dataset(name='band_data')\n",
    "\n",
    "    # normalized the band_data\n",
    "    band_data = xa.band_data.to_numpy().astype(float)\n",
    "    band_data = (\n",
    "        (band_data - np.nanmin(band_data))\n",
    "        * (255 / (np.nanmax(band_data) - np.nanmin(band_data)))\n",
    "        )\n",
    "\n",
    "    # calculate relative greenness\n",
    "    red = band_data[0]\n",
    "    green = band_data[1]\n",
    "    blue = band_data[2]\n",
    "    nir = band_data[3]\n",
    "    rgi = green / (red + green + blue)\n",
    "    xa['rgi'] = (('y', 'x'), rgi)\n",
    "\n",
    "    # calculate pixel by pixel normalized R, G, B, and NIR\n",
    "    rgbn_tot = red + green + blue + nir\n",
    "    xa['red_'] = (('y', 'x'), red  / rgbn_tot)\n",
    "    xa['blue_'] = (('y', 'x'), blue  / rgbn_tot)\n",
    "    xa['green_'] = (('y', 'x'), green  / rgbn_tot)\n",
    "    xa['nir_'] = (('y', 'x'), nir  / rgbn_tot)\n",
    "\n",
    "    # calculate NDVI and SAVI\n",
    "    nir_agg = xa.band_data[3].astype(float)\n",
    "    red_agg = xa.band_data[2].astype(float)\n",
    "    ndvi_agg = ndvi(nir_agg, red_agg)\n",
    "    savi_agg = savi(nir_agg, red_agg)\n",
    "    xa['NDVI'] = ndvi_agg\n",
    "    xa['SAVI'] = savi_agg\n",
    "\n",
    "    # calculate RGB luminosity\n",
    "    luminosity = band_data[:3].mean(axis=0) / 255\n",
    "    xa['luminosity'] = (('y', 'x'), luminosity)\n",
    "\n",
    "    # mask out shadows and soil for RGI,NDVI, and normed pix colors\n",
    "    mask = (luminosity > 0.176) & (luminosity < 0.569)\n",
    "    masked_rgi = xa.rgi.where(mask)\n",
    "    masked_ndvi = xa.NDVI.where(mask)\n",
    "    r_ = xa.red_.where(mask)\n",
    "    g_ = xa.green_.where(mask)\n",
    "    b_ = xa.blue_.where(mask)\n",
    "    n_ = xa.nir_.where(mask)\n",
    "    \n",
    "    data = []\n",
    "    masked_count = 0\n",
    "    total = len(crowns)\n",
    "    bins = np.arange(0.1, 1.1, 0.1)\n",
    "    \n",
    "    for _, row in crowns.iterrows():\n",
    "        # calculate luminosity fractions\n",
    "        lum = xa.luminosity.rio.clip([row.geometry]).to_numpy().flatten()\n",
    "        lum_tot = lum.shape[0]\n",
    "        lum_fracs = [\n",
    "            ((lum < f).sum() - (lum < f - 0.1).sum())\n",
    "            / lum_tot\n",
    "            for f in bins\n",
    "            ]\n",
    "\n",
    "        # calculate rgi fracs\n",
    "        rgi = masked_rgi.rio.clip([row.geometry]).to_numpy().flatten()\n",
    "        rgi = rgi[~np.isnan(rgi)]\n",
    "        rgi_tot = len(rgi)\n",
    "        if rgi_tot == 0:\n",
    "            rgi_fracs = [-99] * 10\n",
    "        else:\n",
    "            rgi_fracs = [\n",
    "                ((rgi < f).sum() - (rgi < f - 0.1).sum())\n",
    "                / rgi_tot\n",
    "                for f in bins\n",
    "                ]\n",
    "            \n",
    "        # clip normed rgbn ro crown\n",
    "        r = r_.rio.clip([row.geometry]).to_numpy().flatten()\n",
    "        r = r[~np.isnan(r)]\n",
    "        \n",
    "        g = g_.rio.clip([row.geometry]).to_numpy().flatten()\n",
    "        g = g[~np.isnan(g)]\n",
    "\n",
    "        b = b_.rio.clip([row.geometry]).to_numpy().flatten()\n",
    "        b = b[~np.isnan(b)]\n",
    "\n",
    "        n = n_.rio.clip([row.geometry]).to_numpy().flatten()\n",
    "        n = n[~np.isnan(n)]\n",
    "\n",
    "        # get number of unmasked pix within crown\n",
    "        c_tot = len(r)\n",
    "\n",
    "        if c_tot == 0:\n",
    "        # if there are no unmasked pix in crown fill with -99\n",
    "            r_fracs = [-99] * 10\n",
    "            g_fracs = [-99] * 10\n",
    "            b_fracs = [-99] * 10\n",
    "            n_fracs = [-99] * 10\n",
    "        else:\n",
    "            # calculate normed pix color fracs\n",
    "            r_fracs = [\n",
    "                ((r < f).sum() - (r < f - 0.1).sum())\n",
    "                / c_tot\n",
    "                for f in bins\n",
    "                ]\n",
    "\n",
    "            g_fracs = [\n",
    "                ((g < f).sum() - (g < f - 0.1).sum())\n",
    "                / c_tot\n",
    "                for f in bins\n",
    "                ]\n",
    "\n",
    "            b_fracs = [\n",
    "                ((b < f).sum() - (b < f - 0.1).sum())\n",
    "                / c_tot\n",
    "                for f in bins\n",
    "                ]\n",
    "\n",
    "            n_fracs = [\n",
    "                ((n < f).sum() - (n < f - 0.1).sum())\n",
    "                / c_tot\n",
    "                for f in bins\n",
    "                ]\n",
    "\n",
    "        # calculate means and stdevs\n",
    "        if rgi_tot == 0:\n",
    "            ndvi_mean, ndvi_std = -99, -99\n",
    "            rgi_mean, rgi_std = -99, -99\n",
    "            savi_mean, savi_std = -99, -99\n",
    "            r_mean, r_std = -99, -99\n",
    "            g_mean, g_std = -99, -99\n",
    "            b_mean, b_std = -99, -99\n",
    "            n_mean, n_std = -99, -99\n",
    "        else:\n",
    "            #NOTE: .values * 1 casts 1 item DataArray to float\n",
    "            ndvi_mean = masked_ndvi.mean().values * 1\n",
    "            ndvi_std = masked_ndvi.std().values * 1\n",
    "            rgi_mean = rgi.mean() \n",
    "            rgi_std = rgi.std()\n",
    "            savi_mean = xa.SAVI.mean().values * 1\n",
    "            savi_std = xa.SAVI.std().values * 1\n",
    "            r_mean, r_std = r.mean(), r.std()\n",
    "            g_mean, g_std = g.mean(), g.std()\n",
    "            b_mean, b_std = b.mean(), b.std()\n",
    "            n_mean, n_std = n.mean(), n.std()\n",
    "\n",
    "        if label is None:\n",
    "            row[label] = -99\n",
    "\n",
    "        data.append(\n",
    "            [row[IDcolumn], (row[label] + 1) / 2] +\n",
    "            lum_fracs +\n",
    "            rgi_fracs + \n",
    "            r_fracs + \n",
    "            g_fracs + \n",
    "            b_fracs + \n",
    "            n_fracs +\n",
    "            [ndvi_mean, ndvi_std, rgi_mean, rgi_std, savi_mean, savi_std] +\n",
    "            [r_mean, r_std, g_mean, g_std, b_mean, b_std, n_mean, n_std]\n",
    "            )\n",
    "\n",
    "    cols = [\n",
    "        IDcolumn, 'label',\n",
    "        'lum10', 'lum20', 'lum30', 'lum40', 'lum50',\n",
    "        'lum60' ,'lum70', 'lum80', 'lum90', 'lum100',\n",
    "        'rgi10', 'rgi20', 'rgi30', 'rgi40', 'rgi50',\n",
    "        'rgi60' ,'rgi70', 'rgi80', 'rgi90', 'rgi100',\n",
    "        'r10', 'r20', 'r30', 'r40', 'r50',\n",
    "        'r60' ,'r70', 'r80', 'r90', 'r100',\n",
    "        'g10', 'g20', 'g30', 'g40', 'g50',\n",
    "        'g60' ,'g70', 'g80', 'g90', 'g100',\n",
    "        'b10', 'b20', 'b30', 'b40', 'b50',\n",
    "        'b60' ,'b70', 'b80', 'b90', 'b100',\n",
    "        'n10', 'n20', 'n30', 'n40', 'n50',\n",
    "        'n60' ,'n70', 'n80', 'n90', 'n100',\n",
    "        'ndvi_mean', 'ndvi_std',\n",
    "        'rgi_mean', 'rgi_std',\n",
    "        'savi_mean', 'savi_std',\n",
    "        'r_mean', 'r_std',\n",
    "        'g_mean', 'g_std',\n",
    "        'b_mean', 'b_std',\n",
    "        'n_mean', 'n_std'\n",
    "        ]\n",
    "\n",
    "    return pd.DataFrame(data, columns=cols)\n",
    "\n",
    "\n",
    "def ll_features(sample_df, tif_path, y, tk, gk, IDcolumn=None):\n",
    "    '''\n",
    "    Wrapper for reading crowns and engineering model features\n",
    "    for use with joblib Parallel.\n",
    "    Returns a tuple\n",
    "    (tk, gk, y, feature_df)\n",
    "    '''\n",
    "    return (\n",
    "        tk,\n",
    "        gk,\n",
    "        y,\n",
    "        make_model_inputs(\n",
    "            sample_df,\n",
    "            tif_path,\n",
    "            IDcolumn=IDcolumn\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def make_features():\n",
    "    '''\n",
    "    Builds features from sample dict.\n",
    "    Returns a dict of feature dfs. dfs are leaves\n",
    "    aranged by keys\n",
    "    [treatment_key][geomorphon_key][year] \n",
    "    '''\n",
    "    # open dict of {labeled_polygon: NAIP} pairs from config file\n",
    "    naip_dir = helena_path / 'NAIP'\n",
    "    IDcolumn='UniqueID'\n",
    "    treatment_keys = dict_of_samples.keys()\n",
    "    geomorphon_keys = range(1,11)\n",
    "\n",
    "    # open crowns and make features \n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(ll_features)(\n",
    "            dict_of_samples[tk][gk],\n",
    "            naip_dir / y / f'{y}.vrt',\n",
    "            y, tk, gk,\n",
    "            IDcolumn=IDcolumn,\n",
    "            )\n",
    "        for y in [2018, 2020, 2022]                         \n",
    "        for tk in treatment_keys\n",
    "        for gk in geomorphon_keys)\n",
    "    \n",
    "    data = dict(results)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = make_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the model which was tuned and trained in `src/mortality_classification.ipynb`.  It was pickled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model created in src/mortality_classification.ipynb\n",
    "pickle_path = Path.cwd() / 'RF_model.sav'\n",
    "model = pickle.load(open(pickle_path, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make model predictions for the samples and create a timeseries of survival probabilities for each sample over the years for which we have NAIP data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_keys = dict_of_samples.keys()\n",
    "geomorphon_keys = range(1,11)\n",
    "years = [2018, 2020, 2022]\n",
    "\n",
    "for tk in treatment_keys:\n",
    "    for gk in geomorphon_keys:\n",
    "        ...\n",
    "def sample_mortality_timeseries(sample_dict, years):\n",
    "    '''\n",
    "    Takes a dict of years for a given sample,\n",
    "    returns a df of probabilities of being alive\n",
    "    by year.\n",
    "    '''\n",
    "    t_series = []\n",
    "    for y in years:\n",
    "        cols = sample_dict[y].drop(['y', 'label', 'UniqueID'], axis=1).columns\n",
    "        X = sample_dict[y][cols]\n",
    "        lil_df = pd.DataFrame()\n",
    "        lil_df['UniqueID'] = sample_dict[y]['UniqueID']\n",
    "        lil_df['pred'] = model.predict_proba(X)[:, 1]\n",
    "        t_series.append(lil_df)\n",
    "        \n",
    "    t_series = [t_series[0].join(df_, on='UniqueID') for df_ in t_series[1:]][0]\n",
    "        \n",
    "    return t_series\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "treemort",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
