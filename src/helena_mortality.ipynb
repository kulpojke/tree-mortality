{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "from statistics import mode\n",
    "\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import rioxarray\n",
    "import cupy\n",
    "\n",
    "from xrspatial import hillshade\n",
    "from xrspatial import convolution\n",
    "from datashader.colors import Set1\n",
    "from datashader.transfer_functions import shade\n",
    "from datashader.transfer_functions import stack\n",
    "from datashader.transfer_functions import dynspread\n",
    "from datashader.transfer_functions import set_background\n",
    "from datashader.colors import Elevation\n",
    "\n",
    "from xrspatial import focal, slope\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from joblib_progress import joblib_progress\n",
    "from xrspatial.multispectral import ndvi, savi\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (confusion_matrix, ConfusionMatrixDisplay)\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV as RSCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "high_high_path = '/home/michael/TreeMortality/data/helena/treatment_polys/code15_n5.gpkg'\n",
    "high_un_path = '/home/michael/TreeMortality/data/helena/treatment_polys/code12_n5.gpkg'\n",
    "un_high_path = '/home/michael/TreeMortality/data/helena/treatment_polys/code3_n5.gpkg'\n",
    "un_un_path = '/home/michael/TreeMortality/data/helena/treatment_polys/code0_n5.gpkg'\n",
    "poly_paths = [high_high_path, high_un_path, un_high_path, un_un_path]\n",
    "\n",
    "\n",
    "helena_path = Path.cwd().parent / 'data' / 'helena'\n",
    "geomorph_dir = helena_path / 'geomorphons'\n",
    "crown_path = helena_path / 'crowns'\n",
    "crown_path_list = [\n",
    "    c for c\n",
    "    in crown_path.iterdir()\n",
    "    if c.suffix == '.gpkg'\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "# open treatment polygons\n",
    "df = pd.concat([gpd.read_file(p) for p in poly_paths])\n",
    "df = df.drop('area_', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first check each crown to see if it falls completely within one of the treatment class areas.  If so it will be appended to a datframe of crowns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jobs to run in ||\n",
    "n_jobs = 23\n",
    "\n",
    "def is_in_treatment(crown_df, row, buf):\n",
    "    '''Returns only crowns with buffer completely within polygon'''\n",
    "    crown_df.loc[\n",
    "        buf.within(row.geometry),\n",
    "        'treatment'] = row.attribute\n",
    "    \n",
    "    return crown_df[crown_df.treatment >= 0 ]\n",
    "\n",
    "\n",
    "def label_treatment(f):\n",
    "    crown_df = gpd.read_file(f)\n",
    "    crown_df = crown_df[crown_df.geometry.area > 10]\n",
    "    \n",
    "    # get total bounds of tile as polygon\n",
    "    bounds = crown_df.total_bounds\n",
    "    bbox = shapely.geometry.box(*bounds)\n",
    "\n",
    "    # use only treatment geometries which touch the tile\n",
    "    sub_df = df[df.geometry.intersects(bbox)]\n",
    "    if len(sub_df) > 0:\n",
    "        # add treatment column\n",
    "        crown_df['treatment'] = -99\n",
    "        #buffer crowns\n",
    "        buf = crown_df.geometry.buffer(10)\n",
    "        # label treatments of crowns lying completely within poly\n",
    "        return Parallel(n_jobs=n_jobs)(\n",
    "            delayed(is_in_treatment)(crown_df, row, buf)\n",
    "            for _, row in sub_df.iterrows()\n",
    "            )\n",
    "    else:\n",
    "        # return empty df, but add treatment column first\n",
    "        cols = list(crown_df.columns) + ['treatment']\n",
    "        empty_df = pd.DataFrame(columns=cols)\n",
    "        return [empty_df]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f996a7f9d7214fc8a74c6b061bb5002b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with joblib_progress('', total=len(crown_path_list)):\n",
    "    results =  Parallel(n_jobs=n_jobs)(delayed(label_treatment)(f) for f in crown_path_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results is a list of lists of dfs, so we must flatten to concat\n",
    "crown_df = pd.concat([item for sublist in results for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "results = [label_treatment(f) for f in tqdm(crown_path_list)]\n",
    "crown_df = pd.concat(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we will also add a unique identifier.  Then save `crowns_df` so in case ware interrupted,  we will be able to resume without running the 5 hour block of code above again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDdalponte</th>\n",
       "      <th>zmax</th>\n",
       "      <th>zmean</th>\n",
       "      <th>zsd</th>\n",
       "      <th>zskew</th>\n",
       "      <th>zkurt</th>\n",
       "      <th>zentropy</th>\n",
       "      <th>pzabovezmean</th>\n",
       "      <th>pzabove2</th>\n",
       "      <th>zq5</th>\n",
       "      <th>...</th>\n",
       "      <th>p2th</th>\n",
       "      <th>p3th</th>\n",
       "      <th>p4th</th>\n",
       "      <th>p5th</th>\n",
       "      <th>pground</th>\n",
       "      <th>n</th>\n",
       "      <th>area</th>\n",
       "      <th>geometry</th>\n",
       "      <th>treatment</th>\n",
       "      <th>UniqueID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.29</td>\n",
       "      <td>4.970000</td>\n",
       "      <td>0.549181</td>\n",
       "      <td>-1.123796</td>\n",
       "      <td>2.307702</td>\n",
       "      <td>0.313845</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.3030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0672</td>\n",
       "      <td>POLYGON ((496566.730 4511249.660, 496566.620 4...</td>\n",
       "      <td>12</td>\n",
       "      <td>10N_496567_4511250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.93</td>\n",
       "      <td>5.684000</td>\n",
       "      <td>0.387337</td>\n",
       "      <td>-1.405252</td>\n",
       "      <td>3.124655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>5.1560</td>\n",
       "      <td>...</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0760</td>\n",
       "      <td>POLYGON ((496570.930 4511249.740, 496570.640 4...</td>\n",
       "      <td>12</td>\n",
       "      <td>10N_496571_4511250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>9.26</td>\n",
       "      <td>7.120000</td>\n",
       "      <td>2.037727</td>\n",
       "      <td>-0.419361</td>\n",
       "      <td>1.899452</td>\n",
       "      <td>0.602060</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.8005</td>\n",
       "      <td>...</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0638</td>\n",
       "      <td>POLYGON ((496589.250 4511249.710, 496589.000 4...</td>\n",
       "      <td>12</td>\n",
       "      <td>10N_496589_4511250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>13.39</td>\n",
       "      <td>11.101111</td>\n",
       "      <td>1.725648</td>\n",
       "      <td>0.357462</td>\n",
       "      <td>1.321602</td>\n",
       "      <td>0.435405</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>100.0</td>\n",
       "      <td>9.4440</td>\n",
       "      <td>...</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.1551</td>\n",
       "      <td>POLYGON ((496743.460 4511249.750, 496743.330 4...</td>\n",
       "      <td>12</td>\n",
       "      <td>10N_496743_4511250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.0</td>\n",
       "      <td>9.49</td>\n",
       "      <td>6.605000</td>\n",
       "      <td>2.987673</td>\n",
       "      <td>-0.013348</td>\n",
       "      <td>1.025824</td>\n",
       "      <td>0.439247</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.6300</td>\n",
       "      <td>...</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.1148</td>\n",
       "      <td>POLYGON ((496775.400 4511249.590, 496775.330 4...</td>\n",
       "      <td>12</td>\n",
       "      <td>10N_496775_4511250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   IDdalponte   zmax      zmean       zsd     zskew     zkurt  zentropy  \\\n",
       "0         2.0   5.29   4.970000  0.549181 -1.123796  2.307702  0.313845   \n",
       "1         3.0   5.93   5.684000  0.387337 -1.405252  3.124655  0.000000   \n",
       "2         4.0   9.26   7.120000  2.037727 -0.419361  1.899452  0.602060   \n",
       "3        10.0  13.39  11.101111  1.725648  0.357462  1.321602  0.435405   \n",
       "4        11.0   9.49   6.605000  2.987673 -0.013348  1.025824  0.439247   \n",
       "\n",
       "   pzabovezmean  pzabove2     zq5  ...       p2th       p3th  p4th  p5th  \\\n",
       "0     75.000000     100.0  4.3030  ...   0.000000   0.000000   0.0   0.0   \n",
       "1     80.000000     100.0  5.1560  ...  20.000000   0.000000   0.0   0.0   \n",
       "2     50.000000     100.0  4.8005  ...  25.000000   0.000000   0.0   0.0   \n",
       "3     44.444444     100.0  9.4440  ...  44.444444   0.000000   0.0   0.0   \n",
       "4     50.000000     100.0  3.6300  ...  50.000000  16.666667   0.0   0.0   \n",
       "\n",
       "   pground  n    area                                           geometry  \\\n",
       "0      0.0  4  0.0672  POLYGON ((496566.730 4511249.660, 496566.620 4...   \n",
       "1      0.0  5  0.0760  POLYGON ((496570.930 4511249.740, 496570.640 4...   \n",
       "2      0.0  4  0.0638  POLYGON ((496589.250 4511249.710, 496589.000 4...   \n",
       "3      0.0  9  0.1551  POLYGON ((496743.460 4511249.750, 496743.330 4...   \n",
       "4      0.0  6  0.1148  POLYGON ((496775.400 4511249.590, 496775.330 4...   \n",
       "\n",
       "   treatment            UniqueID  \n",
       "0         12  10N_496567_4511250  \n",
       "1         12  10N_496571_4511250  \n",
       "2         12  10N_496589_4511250  \n",
       "3         12  10N_496743_4511250  \n",
       "4         12  10N_496775_4511250  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_unique_ID(crowns, utm_zone):\n",
    "    '''\n",
    "    returns copy of dataframe with new uniqueID column\n",
    "    with entries of form 'utm_zone_x_y where x and y \n",
    "    are rounded to the nearest meter.\n",
    "    TODO: make it round to nearest even meter to lower precision\n",
    "    '''\n",
    "    crowns['UniqueID'] = crowns.geometry.centroid.apply(\n",
    "        lambda p: f'{utm_zone}_{p.x:.0f}_{p.y:.0f}')\n",
    "    \n",
    "    return crowns\n",
    "\n",
    "# add unique ID\n",
    "crown_df_ = make_unique_ID(crown_df, '10N')\n",
    "crown_df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "crown_df_.to_file(helena_path / 'crowns_with_treatment_label.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "treatment\n",
       "0     2794929\n",
       "3      601844\n",
       "12     468077\n",
       "15      11980\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure the number of treatments is reasonable\n",
    "crown_df_.treatment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now finally we know it is safe to make crown_df = crown_df_ \n",
    "#crown_df = crown_df_\n",
    "\n",
    "# or load it from file if you were interrupted\n",
    "crown_df = gpd.read_file(helena_path / 'crowns_with_treatment_label.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OMG, I forgot to do this at the begining, probably wasted a lot of time on tiny polys\n",
    "# fixed above for next time\n",
    "crown_df = crown_df[crown_df.geometry.area > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also save fixed version, lets use geoparquet\n",
    "crown_df.to_parquet(helena_path / 'crowns_with_treatment_label.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geomorphons\n",
    "In order to look at the effects of slope position on tree mortality we will use the geomorphons algorithm  as implemented in Whitebox Tools.  We will use the geoporphons rasters that were calculated in `src/helena_geomorphon.ipynb`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "from statistics import mode\n",
    "\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import rioxarray\n",
    "import cupy\n",
    "\n",
    "from xrspatial import hillshade\n",
    "from xrspatial import convolution\n",
    "from datashader.colors import Set1\n",
    "from datashader.transfer_functions import shade\n",
    "from datashader.transfer_functions import stack\n",
    "from datashader.transfer_functions import dynspread\n",
    "from datashader.transfer_functions import set_background\n",
    "from datashader.colors import Elevation\n",
    "\n",
    "from xrspatial import focal, slope\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from joblib_progress import joblib_progress\n",
    "from xrspatial.multispectral import ndvi, savi\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (confusion_matrix, ConfusionMatrixDisplay)\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV as RSCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, log_loss\n",
    "\n",
    "# paths\n",
    "high_high_path = '/home/michael/TreeMortality/data/helena/treatment_polys/code15_n5.gpkg'\n",
    "high_un_path = '/home/michael/TreeMortality/data/helena/treatment_polys/code12_n5.gpkg'\n",
    "un_high_path = '/home/michael/TreeMortality/data/helena/treatment_polys/code3_n5.gpkg'\n",
    "un_un_path = '/home/michael/TreeMortality/data/helena/treatment_polys/code0_n5.gpkg'\n",
    "poly_paths = [high_high_path, high_un_path, un_high_path, un_un_path]\n",
    "\n",
    "\n",
    "helena_path = Path.cwd().parent / 'data' / 'helena'\n",
    "geomorph_dir = helena_path / 'geomorphons'\n",
    "crown_path = helena_path / 'crowns'\n",
    "crown_path_list = [\n",
    "    c for c\n",
    "    in crown_path.iterdir()\n",
    "    if c.suffix == '.gpkg'\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "treatment\n",
       "0     2666168\n",
       "3      527050\n",
       "12     415553\n",
       "15       6409\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or open the parquet,  way faster\n",
    "crown_df = gpd.read_parquet(helena_path / 'crowns_with_treatment_label.parquet')\n",
    "crown_df.treatment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 51409\n",
      "Value counts:\n",
      "treatment\n",
      "12    15000\n",
      "3     15000\n",
      "0     15000\n",
      "15     6409\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "samp_size = 15_000\n",
    "crown_df = pd.concat(\n",
    "    [\n",
    "        crown_df[crown_df.treatment == 15],\n",
    "        crown_df[crown_df.treatment == 12].sample(samp_size, random_state=1),\n",
    "        crown_df[crown_df.treatment == 3].sample(samp_size, random_state=1),\n",
    "        crown_df[crown_df.treatment == 0].sample(samp_size, random_state=1)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "print(f'total: {len(crown_df)}\\nValue counts:')\n",
    "print(crown_df.treatment.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:37<00:00, 55.43s/it]\n"
     ]
    }
   ],
   "source": [
    "for r in tqdm([100, 250, 500, 1000, 2000]):\n",
    "    # open geomorphon tif\n",
    "    tif = geomorph_dir / f'geomorph_{r}.tif' \n",
    "    gmorph = rioxarray.open_rasterio(tif)\n",
    "      \n",
    "    # attach landform to crowns\n",
    "    centroids = [(c.x, c.y) for c in crown_df.geometry.centroid.to_list()]\n",
    "    crown_df[f'geomorph_{r}'] = [gmorph.sel(x=x, y=y, method='nearest').item() for x, y in centroids]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to make sure all the scales have the same landforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "geomorph_cols = [col for col in crown_df.columns if 'geomorph' in col]\n",
    "landforms = set(crown_df[geomorph_cols[0]].to_list())\n",
    "\n",
    "for col in geomorph_cols:\n",
    "    v = set(crown_df[col].to_list())\n",
    "    try:\n",
    "        assert v == landforms\n",
    "    except AssertionError:\n",
    "        print(f'{geomorph_cols[0]} and {col} have different unique landforms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a two-level stratified sample of crowns based on treatment and geomorphon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_samples(r):\n",
    "    # dict to hold samples\n",
    "    dict_of_samples = {}\n",
    "\n",
    "    # split into groups based on treatment\n",
    "    for tr in [0, 3, 12, 15]:\n",
    "        df_x = crown_df[crown_df.treatment == tr]\n",
    "\n",
    "        # for each treatment, split based on landform.\n",
    "        sub_dict = {}\n",
    "        for pos in landforms:\n",
    "            sub_dict[f'geomorph_{r}'] = df_x[df_x[f'geomorph_{r}'] == pos]\n",
    "        dict_of_samples[f'treatment_{tr}'] = sub_dict\n",
    "\n",
    "    # find the size of the smallest smallest treatment/landform population\n",
    "    n = np.inf\n",
    "    for key1, sub_dict in dict_of_samples.items():\n",
    "        \n",
    "        lengths = [len(d) for d in dict_of_samples[key1]]\n",
    "        n = min(n, min(lengths))\n",
    "    '''         \n",
    "    print(f'For {r}, ')    \n",
    "    print(f'the smallest treatment/landform population is {n}.')\n",
    "    print(f'It is treatment {tr}')\n",
    "    '''\n",
    "    return(dict_of_samples)\n",
    "\n",
    "    \n",
    "# dict to hold dicts of samples for each geomorphon scale\n",
    "dict_of_scales = {}\n",
    "for r in [100, 250, 500, 1000, 2000]:\n",
    "    dict_of_scales[f'geomorph_{r}'] = make_samples(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>treatment_code</th>\n",
       "      <th>0</th>\n",
       "      <th>3</th>\n",
       "      <th>12</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scale</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>100</td>\n",
       "      <td>33</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>255</td>\n",
       "      <td>82</td>\n",
       "      <td>51</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>443</td>\n",
       "      <td>163</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>831</td>\n",
       "      <td>290</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>1256</td>\n",
       "      <td>551</td>\n",
       "      <td>417</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "treatment_code    0    3    12  15\n",
       "scale                             \n",
       "100              100   33   31   0\n",
       "250              255   82   51   8\n",
       "500              443  163  113   0\n",
       "1000             831  290  236   0\n",
       "2000            1256  551  417   0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "for r in [100, 250, 500, 1000, 2000]:\n",
    "    for tr in [0, 3, 12, 15]:\n",
    "        length = len(dict_of_scales[f'geomorph_{r}'][f'treatment_{tr}'][f'geomorph_{r}'])\n",
    "        rows.append((r, tr, length))\n",
    "\n",
    "sample_stats = pd.DataFrame(rows, columns=['scale', 'treatment_code', 'n_samples'])\n",
    "sample_stats.pivot(index='scale', columns='treatment_code', values='n_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {}\n",
    "\n",
    "for r in [100, 250, 500, 1000, 2000]:\n",
    "    samps = []\n",
    "    for tr in [0, 3, 12, 15]:\n",
    "        # just hardcoded the tr value for sample size based on sample_stats\n",
    "        sample_size = len(dict_of_scales[f'geomorph_{r}'][f'treatment_{3}'][f'geomorph_{r}'])\n",
    "        # get desired dataframe\n",
    "        df_ = dict_of_scales[f'geomorph_{r}'][f'treatment_{tr}'][f'geomorph_{r}']\n",
    "        # if df_ is larger than sample_size, reduce it to sample_size\n",
    "        if len(df_) >= sample_size:\n",
    "            df_ = df_.sample(sample_size, random_state=123)\n",
    "            \n",
    "        # stick in list\n",
    "        samps.append(df_)\n",
    "    # concat dfs\n",
    "    samps = pd.concat(samps)\n",
    "        \n",
    "    # put samps into the samples dict\n",
    "    samples[f'geomorph_{r}'] = samps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>treatment_code</th>\n",
       "      <th>0</th>\n",
       "      <th>3</th>\n",
       "      <th>12</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scale</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>51</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>163</td>\n",
       "      <td>163</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>290</td>\n",
       "      <td>290</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>551</td>\n",
       "      <td>551</td>\n",
       "      <td>417</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "treatment_code   0    3    12  15\n",
       "scale                            \n",
       "100              33   33   31   0\n",
       "250              82   82   51   8\n",
       "500             163  163  113   0\n",
       "1000            290  290  236   0\n",
       "2000            551  551  417   0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "for r in [100, 250, 500, 1000, 2000]:\n",
    "    df_ = samples[f'geomorph_{r}']\n",
    "    for tr in [0, 3, 12, 15]:\n",
    "        length = len(df_[df_.treatment == tr])\n",
    "        rows.append((r, tr, length))\n",
    "\n",
    "sample_stats = pd.DataFrame(rows, columns=['scale', 'treatment_code', 'n_samples'])\n",
    "sample_stats.pivot(index='scale', columns='treatment_code', values='n_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to engineer the features for our model, as done in `src/mortality_classification.ipynb`.  Because the Naip imagery for the area is so large, in order for this to run on my machine, the dtype of the imagery has been changed to `np.float32`.  This should not make a difference in the results, but is worth noting.  in `mortality_classification_geographic_holdouts.ipynb` feature creation was run in parallel, but tha tis not possible with sucha large image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_inputs(crowns, tif, save_path, y, gk, label=None, IDcolumn=None):\n",
    "    '''\n",
    "    Returns DataFrame with features for use in classification model.\n",
    "    The resulting DataFrame has 'ID' column which matches that in crowns.\n",
    "    The DataFrame also has a 'label' column, see params for more detail.  \n",
    "\n",
    "    params:\n",
    "        crowns   - str - path to OGR readable vector file containing tree crowns.\n",
    "        tif      - xr  - image used in producing features, already read\n",
    "                         with rioxarray.\n",
    "        label    - str - specifies column containing labels.  If specified 'label'\n",
    "                         column in resulting DataFrame will contain contents of \n",
    "                         specified column. Otherwise 'label' column contain -99.\n",
    "        IDcolumn - str - column to use as matching ID with crowns\n",
    "    '''\n",
    "    try:\n",
    "        # get the extent of the crowns\n",
    "        xmin, ymin, xmax, ymax = crowns.total_bounds\n",
    "\n",
    "        # clip the naip image\n",
    "        print(f'\\t\\t--clipping...')\n",
    "        xa = tif.astype(np.float32).rio.clip_box(\n",
    "            minx=xmin,\n",
    "            miny=ymin,\n",
    "            maxx=xmax,\n",
    "            maxy=ymax\n",
    "            ).to_dataset(name='band_data')\n",
    "\n",
    "        # normalized the band_data\n",
    "        print(f'\\t\\t--normalizing...')\n",
    "        band_data = xa.band_data.to_numpy().astype(np.float32)\n",
    "        band_data = (band_data - np.nanmin(band_data)) * (255 / (np.nanmax(band_data) - np.nanmin(band_data)))\n",
    "\n",
    "        # calculate relative greenness\n",
    "        print(f'\\t\\t--calculating RGI...')\n",
    "        red = band_data[0]\n",
    "        green = band_data[1]\n",
    "        blue = band_data[2]\n",
    "        nir = band_data[3]\n",
    "        rgi = green / (red + green + blue)\n",
    "        xa['rgi'] = (('y', 'x'), rgi)\n",
    "\n",
    "        # calculate pixel by pixel normalized R, G, B, and NIR\n",
    "        print(f'\\t\\t--pix norming...')\n",
    "        rgbn_tot = red + green + blue + nir\n",
    "        xa['red_'] = (('y', 'x'), red  / rgbn_tot)\n",
    "        xa['blue_'] = (('y', 'x'), blue  / rgbn_tot)\n",
    "        xa['green_'] = (('y', 'x'), green  / rgbn_tot)\n",
    "        xa['nir_'] = (('y', 'x'), nir  / rgbn_tot)\n",
    "\n",
    "        # calculate NDVI and SAVI\n",
    "        print(f'\\t\\t--NDVI and SAVI...')\n",
    "        nir_agg = xa.band_data[3].astype(np.float32)\n",
    "        red_agg = xa.band_data[2].astype(np.float32)\n",
    "        ndvi_agg = ndvi(nir_agg, red_agg)\n",
    "        savi_agg = savi(nir_agg, red_agg)\n",
    "        xa['NDVI'] = ndvi_agg\n",
    "        xa['SAVI'] = savi_agg\n",
    "\n",
    "        # calculate RGB luminosity\n",
    "        print(f'\\t\\t--luminosity...')\n",
    "        luminosity = band_data[:3].mean(axis=0) / 255\n",
    "        xa['luminosity'] = (('y', 'x'), luminosity)\n",
    "\n",
    "        # mask out shadows and soil for RGI,NDVI, and normed pix colors\n",
    "        print(f'\\t\\t--masking...')\n",
    "        mask = (luminosity > 0.176) & (luminosity < 0.569)\n",
    "        masked_rgi = xa.rgi.where(mask)\n",
    "        masked_ndvi = xa.NDVI.where(mask)\n",
    "        r_ = xa.red_.where(mask)\n",
    "        g_ = xa.green_.where(mask)\n",
    "        b_ = xa.blue_.where(mask)\n",
    "        n_ = xa.nir_.where(mask)\n",
    "        \n",
    "        print(f'\\t\\t--adding index data...')\n",
    "        data = []\n",
    "        masked_count = 0\n",
    "        total = len(crowns)\n",
    "        bins = np.arange(0.1, 1.1, 0.1)\n",
    "        with tqdm(total=total) as progress_bar:\n",
    "            for _, row in crowns.iterrows():\n",
    "                # calculate luminosity fractions\n",
    "                lum = xa.luminosity.rio.clip([row.geometry]).to_numpy().flatten()\n",
    "                lum_tot = lum.shape[0]\n",
    "                lum_fracs = [((lum < f).sum() - (lum < f - 0.1).sum()) / lum_tot for f in bins]\n",
    "\n",
    "                # calculate rgi fracs\n",
    "                rgi = masked_rgi.rio.clip([row.geometry]).to_numpy().flatten()\n",
    "                rgi = rgi[~np.isnan(rgi)]\n",
    "                rgi_tot = len(rgi)\n",
    "                if rgi_tot == 0:\n",
    "                    rgi_fracs = [-99] * 10\n",
    "                else:\n",
    "                    rgi_fracs = [((rgi < f).sum() - (rgi < f - 0.1).sum()) / rgi_tot for f in bins]\n",
    "                    \n",
    "                # and normed pix colr fracs\n",
    "                r = r_.rio.clip([row.geometry]).to_numpy().flatten()\n",
    "                r = r[~np.isnan(r)]\n",
    "                c_tot = len(r)\n",
    "                \n",
    "                g = g_.rio.clip([row.geometry]).to_numpy().flatten()\n",
    "                g = g[~np.isnan(g)]\n",
    "\n",
    "                b = b_.rio.clip([row.geometry]).to_numpy().flatten()\n",
    "                b = b[~np.isnan(b)]\n",
    "\n",
    "                n = n_.rio.clip([row.geometry]).to_numpy().flatten()\n",
    "                n = n[~np.isnan(n)]\n",
    "\n",
    "                if c_tot == 0:\n",
    "                    r_fracs = [-99] * 10\n",
    "                    g_fracs = [-99] * 10\n",
    "                    b_fracs = [-99] * 10\n",
    "                    n_fracs = [-99] * 10\n",
    "                else:\n",
    "                    r_fracs = [((r < f).sum() - (r < f - 0.1).sum()) / c_tot for f in bins]\n",
    "                    g_fracs = [((g < f).sum() - (g < f - 0.1).sum()) / c_tot for f in bins]\n",
    "                    b_fracs = [((b < f).sum() - (b < f - 0.1).sum()) / c_tot for f in bins]\n",
    "                    n_fracs = [((n < f).sum() - (n < f - 0.1).sum()) / c_tot for f in bins]\n",
    "                            \n",
    "                # calculate means and stdevs\n",
    "                if rgi_tot == 0:\n",
    "                    ndvi_mean, ndvi_std = -99, -99\n",
    "                    rgi_mean, rgi_std = -99, -99\n",
    "                    savi_mean, savi_std = -99, -99\n",
    "                    r_mean, r_std = -99, -99\n",
    "                    g_mean, g_std = -99, -99\n",
    "                    b_mean, b_std = -99, -99\n",
    "                    n_mean, n_std = -99, -99\n",
    "                else:\n",
    "                    #NOTE: .values * 1 casts 1 item DataArray to float\n",
    "                    ndvi_mean, ndvi_std = masked_ndvi.mean().values * 1, masked_ndvi.std().values * 1\n",
    "                    rgi_mean, rgi_std = rgi.mean(), rgi.std()\n",
    "                    savi_mean, savi_std = xa.SAVI.mean().values * 1, xa.SAVI.std().values * 1\n",
    "                    r_mean, r_std = r.mean(), r.std()\n",
    "                    g_mean, g_std = g.mean(), g.std()\n",
    "                    b_mean, b_std = b.mean(), b.std()\n",
    "                    n_mean, n_std = n.mean(), n.std()\n",
    "\n",
    "                if label is None:\n",
    "                    row[label] = -99\n",
    "\n",
    "                data.append(\n",
    "                    [row[IDcolumn], (row[label] + 1) / 2] +\n",
    "                    lum_fracs +\n",
    "                    rgi_fracs + \n",
    "                    r_fracs + \n",
    "                    g_fracs + \n",
    "                    b_fracs + \n",
    "                    n_fracs +\n",
    "                    [ndvi_mean, ndvi_std, rgi_mean, rgi_std, savi_mean, savi_std] +\n",
    "                    [r_mean, r_std, g_mean, g_std, b_mean, b_std, n_mean, n_std]\n",
    "                    )\n",
    "\n",
    "                #count polygon if has masked pixels            \n",
    "                if rgi_tot < len(xa.rgi.rio.clip([row.geometry]).to_numpy().flatten()):\n",
    "                    masked_count = masked_count + 1\n",
    "\n",
    "                progress_bar.update(1)\n",
    "\n",
    "        cols = [IDcolumn, 'label',\n",
    "                'lum10', 'lum20', 'lum30', 'lum40', 'lum50', 'lum60' ,'lum70', 'lum80', 'lum90', 'lum100',\n",
    "                'rgi10', 'rgi20', 'rgi30', 'rgi40', 'rgi50', 'rgi60' ,'rgi70', 'rgi80', 'rgi90', 'rgi100',\n",
    "                'r10', 'r20', 'r30', 'r40', 'r50', 'r60' ,'r70', 'r80', 'r90', 'r100',\n",
    "                'g10', 'g20', 'g30', 'g40', 'g50', 'g60' ,'g70', 'g80', 'g90', 'g100',\n",
    "                'b10', 'b20', 'b30', 'b40', 'b50', 'b60' ,'b70', 'b80', 'b90', 'b100',\n",
    "                'n10', 'n20', 'n30', 'n40', 'n50', 'n60' ,'n70', 'n80', 'n90', 'n100',\n",
    "                'ndvi_mean', 'ndvi_std', 'rgi_mean', 'rgi_std', 'savi_mean', 'savi_std',\n",
    "                'r_mean', 'r_std', 'g_mean', 'g_std', 'b_mean', 'b_std', 'n_mean', 'n_std']\n",
    "\n",
    "        data = pd.DataFrame(data, columns=cols)\n",
    "        data.to_parquet(save_path / f'features_{y}_{gk}.parquet')\n",
    "        print(y, gk, 'saved to ', str(helena_path / f'features_{y}_{gk}.parquet'))\n",
    "        del data\n",
    "    except:\n",
    "        print(f'OH NO!!! {y}, {gk} FAILED!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------2018-----------\n",
      "\t------geomorph_100/5--------\n",
      "\t\t--clipping...\n",
      "\t\t--normalizing...\n",
      "\t\t--calculating RGI...\n",
      "\t\t--pix norming...\n",
      "\t\t--NDVI and SAVI...\n",
      "\t\t--luminosity...\n",
      "\t\t--masking...\n",
      "\t\t--adding index data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/97 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "naip_dir = helena_path / 'NAIP'\n",
    "save_path = helena_path / 'features'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "def treatment_keys(gk):\n",
    "        if gk == 250:\n",
    "            return [0, 3, 12, 15]\n",
    "        return [3, 12, 15]\n",
    "\n",
    "features = {}\n",
    "\n",
    "for y in [2018, 2020, 2022]:\n",
    "    print(f'----------{y}-----------')                   \n",
    "    tif_path = naip_dir / str(y) / f'{y}.vrt'\n",
    "    xa = rioxarray.open_rasterio(tif_path)\n",
    "    features[y] = {}\n",
    "    for gk in samples.keys():\n",
    "        print(f'\\t------{gk}/{len(samples.keys())}--------')                   \n",
    "        features[y][gk] = make_model_inputs(\n",
    "            samples[gk],\n",
    "            xa,\n",
    "            save_path, y, gk,\n",
    "            label=None,\n",
    "            IDcolumn='UniqueID'\n",
    "            )\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the model which was tuned and trained in `src/mortality_classification.ipynb`.  It was pickled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model created in src/mortality_classification.ipynb\n",
    "pickle_path = Path.cwd() / 'RF_model.sav'\n",
    "model = pickle.load(open(pickle_path, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make model predictions for the samples and create a timeseries of survival probabilities for each sample over the years for which we have NAIP data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_keys = dict_of_samples.keys()\n",
    "geomorphon_keys = range(1,11)\n",
    "years = [2018, 2020, 2022]\n",
    "\n",
    "for tk in treatment_keys:\n",
    "    for gk in geomorphon_keys:\n",
    "        ...\n",
    "def sample_mortality_timeseries(sample_dict, years):\n",
    "    '''\n",
    "    Takes a dict of years for a given sample,\n",
    "    returns a df of probabilities of being alive\n",
    "    by year.\n",
    "    '''\n",
    "    t_series = []\n",
    "    for y in years:\n",
    "        cols = sample_dict[y].drop(['y', 'label', 'UniqueID'], axis=1).columns\n",
    "        X = sample_dict[y][cols]\n",
    "        lil_df = pd.DataFrame()\n",
    "        lil_df['UniqueID'] = sample_dict[y]['UniqueID']\n",
    "        lil_df['pred'] = model.predict_proba(X)[:, 1]\n",
    "        t_series.append(lil_df)\n",
    "        \n",
    "    t_series = [t_series[0].join(df_, on='UniqueID') for df_ in t_series[1:]][0]\n",
    "        \n",
    "    return t_series\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "treemort",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
