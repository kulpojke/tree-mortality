{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/TreeMortality/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import rioxarray\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from xrspatial.multispectral import ndvi, savi\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (confusion_matrix, ConfusionMatrixDisplay)\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV as RSCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, log_loss\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_unique_ID(crowns, utm_zone):\n",
    "    '''\n",
    "    returns copy of dataframe with new uniqueID column\n",
    "    with entries of form 'utm_zone_x_y where x and y \n",
    "    are rounded to the nearest meter.\n",
    "    TODO: make it round to nearest even meter to lower precision\n",
    "    '''\n",
    "    crowns['UniqueID'] = crowns.geometry.centroid.apply(\n",
    "        lambda p: f'{utm_zone}_{p.x:.0f}_{p.y:.0f}')\n",
    "    \n",
    "    return crowns\n",
    "\n",
    "\n",
    "def make_model_inputs(crowns, tif_path, label=None, IDcolumn=None):\n",
    "    '''\n",
    "    Returns DataFrame with features for use in classification model.\n",
    "    The resulting DataFrame has 'ID' column which matches that in crowns.\n",
    "    The DataFrame also has a 'label' column, see params for more detail.  \n",
    "\n",
    "    params:\n",
    "        crowns   - str - path to OGR readable vector file containing tree crowns.\n",
    "        tif_path - str - path to image tif used in producing features.\n",
    "        label    - str - specifies column containing labels.  If specified 'label'\n",
    "                         column in resulting DataFrame will contain contents of \n",
    "                         specified column. Otherwise 'label' column contain -99.\n",
    "        IDcolumn - str - column to use as matching ID with crowns\n",
    "    '''\n",
    "\n",
    "    # get the extent of the crowns\n",
    "    xmin, ymin, xmax, ymax = crowns.total_bounds\n",
    "\n",
    "    # open the naip image\n",
    "    xa = rioxarray.open_rasterio(tif_path).astype(float).rio.clip_box(\n",
    "        minx=xmin,\n",
    "        miny=ymin,\n",
    "        maxx=xmax,\n",
    "        maxy=ymax\n",
    "        ).to_dataset(name='band_data')\n",
    "\n",
    "    # normalized the band_data\n",
    "    band_data = xa.band_data.to_numpy().astype(float)\n",
    "    band_data = (\n",
    "        (band_data - np.nanmin(band_data))\n",
    "        * (255 / (np.nanmax(band_data) - np.nanmin(band_data)))\n",
    "        )\n",
    "\n",
    "    # calculate relative greenness\n",
    "    red = band_data[0]\n",
    "    green = band_data[1]\n",
    "    blue = band_data[2]\n",
    "    nir = band_data[3]\n",
    "    rgi = green / (red + green + blue)\n",
    "    xa['rgi'] = (('y', 'x'), rgi)\n",
    "\n",
    "    # calculate pixel by pixel normalized R, G, B, and NIR\n",
    "    rgbn_tot = red + green + blue + nir\n",
    "    xa['red_'] = (('y', 'x'), red  / rgbn_tot)\n",
    "    xa['blue_'] = (('y', 'x'), blue  / rgbn_tot)\n",
    "    xa['green_'] = (('y', 'x'), green  / rgbn_tot)\n",
    "    xa['nir_'] = (('y', 'x'), nir  / rgbn_tot)\n",
    "\n",
    "    # calculate NDVI and SAVI\n",
    "    nir_agg = xa.band_data[3].astype(float)\n",
    "    red_agg = xa.band_data[2].astype(float)\n",
    "    ndvi_agg = ndvi(nir_agg, red_agg)\n",
    "    savi_agg = savi(nir_agg, red_agg)\n",
    "    xa['NDVI'] = ndvi_agg\n",
    "    xa['SAVI'] = savi_agg\n",
    "\n",
    "    # calculate RGB luminosity\n",
    "    luminosity = band_data[:3].mean(axis=0) / 255\n",
    "    xa['luminosity'] = (('y', 'x'), luminosity)\n",
    "\n",
    "    # mask out shadows and soil for RGI,NDVI, and normed pix colors\n",
    "    mask = (luminosity > 0.176) & (luminosity < 0.569)\n",
    "    masked_rgi = xa.rgi.where(mask)\n",
    "    masked_ndvi = xa.NDVI.where(mask)\n",
    "    r_ = xa.red_.where(mask)\n",
    "    g_ = xa.green_.where(mask)\n",
    "    b_ = xa.blue_.where(mask)\n",
    "    n_ = xa.nir_.where(mask)\n",
    "    \n",
    "    data = []\n",
    "    masked_count = 0\n",
    "    total = len(crowns)\n",
    "    bins = np.arange(0.1, 1.1, 0.1)\n",
    "    \n",
    "    for _, row in crowns.iterrows():\n",
    "        # calculate luminosity fractions\n",
    "        lum = xa.luminosity.rio.clip([row.geometry]).to_numpy().flatten()\n",
    "        lum_tot = lum.shape[0]\n",
    "        lum_fracs = [\n",
    "            ((lum < f).sum() - (lum < f - 0.1).sum())\n",
    "            / lum_tot\n",
    "            for f in bins\n",
    "            ]\n",
    "\n",
    "        # calculate rgi fracs\n",
    "        rgi = masked_rgi.rio.clip([row.geometry]).to_numpy().flatten()\n",
    "        rgi = rgi[~np.isnan(rgi)]\n",
    "        rgi_tot = len(rgi)\n",
    "        if rgi_tot == 0:\n",
    "            rgi_fracs = [-99] * 10\n",
    "        else:\n",
    "            rgi_fracs = [\n",
    "                ((rgi < f).sum() - (rgi < f - 0.1).sum())\n",
    "                / rgi_tot\n",
    "                for f in bins\n",
    "                ]\n",
    "            \n",
    "        # clip normed rgbn ro crown\n",
    "        r = r_.rio.clip([row.geometry]).to_numpy().flatten()\n",
    "        r = r[~np.isnan(r)]\n",
    "        \n",
    "        g = g_.rio.clip([row.geometry]).to_numpy().flatten()\n",
    "        g = g[~np.isnan(g)]\n",
    "\n",
    "        b = b_.rio.clip([row.geometry]).to_numpy().flatten()\n",
    "        b = b[~np.isnan(b)]\n",
    "\n",
    "        n = n_.rio.clip([row.geometry]).to_numpy().flatten()\n",
    "        n = n[~np.isnan(n)]\n",
    "\n",
    "        # get number of unmasked pix within crown\n",
    "        c_tot = len(r)\n",
    "\n",
    "        if c_tot == 0:\n",
    "        # if there are no unmasked pix in crown fill with -99\n",
    "            r_fracs = [-99] * 10\n",
    "            g_fracs = [-99] * 10\n",
    "            b_fracs = [-99] * 10\n",
    "            n_fracs = [-99] * 10\n",
    "        else:\n",
    "            # calculate normed pix color fracs\n",
    "            r_fracs = [\n",
    "                ((r < f).sum() - (r < f - 0.1).sum())\n",
    "                / c_tot\n",
    "                for f in bins\n",
    "                ]\n",
    "\n",
    "            g_fracs = [\n",
    "                ((g < f).sum() - (g < f - 0.1).sum())\n",
    "                / c_tot\n",
    "                for f in bins\n",
    "                ]\n",
    "\n",
    "            b_fracs = [\n",
    "                ((b < f).sum() - (b < f - 0.1).sum())\n",
    "                / c_tot\n",
    "                for f in bins\n",
    "                ]\n",
    "\n",
    "            n_fracs = [\n",
    "                ((n < f).sum() - (n < f - 0.1).sum())\n",
    "                / c_tot\n",
    "                for f in bins\n",
    "                ]\n",
    "\n",
    "        # calculate means and stdevs\n",
    "        if rgi_tot == 0:\n",
    "            ndvi_mean, ndvi_std = -99, -99\n",
    "            rgi_mean, rgi_std = -99, -99\n",
    "            savi_mean, savi_std = -99, -99\n",
    "            r_mean, r_std = -99, -99\n",
    "            g_mean, g_std = -99, -99\n",
    "            b_mean, b_std = -99, -99\n",
    "            n_mean, n_std = -99, -99\n",
    "        else:\n",
    "            #NOTE: .values * 1 casts 1 item DataArray to float\n",
    "            ndvi_mean = masked_ndvi.mean().values * 1\n",
    "            ndvi_std = masked_ndvi.std().values * 1\n",
    "            rgi_mean = rgi.mean() \n",
    "            rgi_std = rgi.std()\n",
    "            savi_mean = xa.SAVI.mean().values * 1\n",
    "            savi_std = xa.SAVI.std().values * 1\n",
    "            r_mean, r_std = r.mean(), r.std()\n",
    "            g_mean, g_std = g.mean(), g.std()\n",
    "            b_mean, b_std = b.mean(), b.std()\n",
    "            n_mean, n_std = n.mean(), n.std()\n",
    "\n",
    "        if label is None:\n",
    "            row[label] = -99\n",
    "\n",
    "        data.append(\n",
    "            [row[IDcolumn], (row[label] + 1) / 2] +\n",
    "            lum_fracs +\n",
    "            rgi_fracs + \n",
    "            r_fracs + \n",
    "            g_fracs + \n",
    "            b_fracs + \n",
    "            n_fracs +\n",
    "            [ndvi_mean, ndvi_std, rgi_mean, rgi_std, savi_mean, savi_std] +\n",
    "            [r_mean, r_std, g_mean, g_std, b_mean, b_std, n_mean, n_std]\n",
    "            )\n",
    "\n",
    "    cols = [\n",
    "        IDcolumn, 'label',\n",
    "        'lum10', 'lum20', 'lum30', 'lum40', 'lum50',\n",
    "        'lum60' ,'lum70', 'lum80', 'lum90', 'lum100',\n",
    "        'rgi10', 'rgi20', 'rgi30', 'rgi40', 'rgi50',\n",
    "        'rgi60' ,'rgi70', 'rgi80', 'rgi90', 'rgi100',\n",
    "        'r10', 'r20', 'r30', 'r40', 'r50',\n",
    "        'r60' ,'r70', 'r80', 'r90', 'r100',\n",
    "        'g10', 'g20', 'g30', 'g40', 'g50',\n",
    "        'g60' ,'g70', 'g80', 'g90', 'g100',\n",
    "        'b10', 'b20', 'b30', 'b40', 'b50',\n",
    "        'b60' ,'b70', 'b80', 'b90', 'b100',\n",
    "        'n10', 'n20', 'n30', 'n40', 'n50',\n",
    "        'n60' ,'n70', 'n80', 'n90', 'n100',\n",
    "        'ndvi_mean', 'ndvi_std',\n",
    "        'rgi_mean', 'rgi_std',\n",
    "        'savi_mean', 'savi_std',\n",
    "        'r_mean', 'r_std',\n",
    "        'g_mean', 'g_std',\n",
    "        'b_mean', 'b_std',\n",
    "        'n_mean', 'n_std'\n",
    "        ]\n",
    "\n",
    "    return pd.DataFrame(data, columns=cols)\n",
    "\n",
    "\n",
    "def ll_features(gpkg, tif_path, label=None, IDcolumn=None, label_col='label'):\n",
    "    '''\n",
    "    Wrapper for reading crowns and engineering model features\n",
    "    for use with joblib Parallel.\n",
    "    Returns a tuple containing the tile identifier (str) and \n",
    "    Pandas df of model input features.\n",
    "    '''\n",
    "    # get tile identifier\n",
    "    tile = (os.path.basename(gpkg)).split('_labels')[0]\n",
    "    # read the crown polygons\n",
    "    crowns = gpd.read_file(gpkg).dropna(\n",
    "        subset=[label_col]\n",
    "        ).set_index('IDdalponte', drop=False)\n",
    "    crowns = crowns[crowns.geometry.area > 10]\n",
    "    \n",
    "    # make uniqueID\n",
    "    crowns = make_unique_ID(crowns, 10)\n",
    "    \n",
    "    return (tile, make_model_inputs(crowns, tif_path, label=label_col, IDcolumn=IDcolumn))\n",
    "\n",
    "\n",
    "def make_features():\n",
    "    '''\n",
    "    Builds features from training data.\n",
    "    Returns a tuple containing the tile identifier (str) and \n",
    "    Pandas df of model input features.\n",
    "    '''\n",
    "    # open dict of {labeled_polygon: NAIP} pairs from config file\n",
    "    with open(Path(os.getcwd()) / 'model_config.json') as src:\n",
    "        config = json.load(src)\n",
    "\n",
    "    label_col = 'label'\n",
    "\n",
    "    # open crowns and make features TODO: unhardcode n_jobs using args\n",
    "    results = Parallel(n_jobs=3)(delayed(ll_features)(\n",
    "        gpkg,\n",
    "        tif,\n",
    "        label=label_col,\n",
    "        IDcolumn='UniqueID')\n",
    "        for gpkg, tif in config.items()\n",
    "    )\n",
    "    data = dict(results)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning 1: /home/michael/TreeMortality/data/huc180102111102/NAIP/2020/m_4012316_sw_10_060_20200710.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: /home/michael/TreeMortality/data/huc180102111102/NAIP/2020/m_4012324_nw_10_060_20200710.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: /home/michael/TreeMortality/data/huc180102111102/NAIP/2020/m_4012324_sw_10_060_20200710.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "/home/michael/TreeMortality/.venv/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "Warning 1: /home/michael/TreeMortality/data/huc180102110901and2and3/NAIP/2022/m_4012308_ne_10_060_20220718.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: /home/michael/TreeMortality/data/huc180102110901and2and3/NAIP/2022/m_4112363_ne_10_060_20220718.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: /home/michael/TreeMortality/data/huc180102110901and2and3/NAIP/2022/m_4112363_nw_10_060_20220718.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: /home/michael/TreeMortality/data/huc180102110901and2and3/NAIP/2022/m_4112363_se_10_060_20220718.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: /home/michael/TreeMortality/data/huc180102110901and2and3/NAIP/2022/m_4112363_sw_10_060_20220718.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: /home/michael/TreeMortality/data/huc180102110901and2and3/NAIP/2022/m_4112364_se_10_060_20220718.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n"
     ]
    }
   ],
   "source": [
    "data = make_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['n_estimators', 'max_features', 'max_depth', 'np.random.seed', 'columns'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the model params determined in `mortality_classification.ipynb`\n",
    "with open('model_params.json') as src:\n",
    "    params = json.load(src)\n",
    "params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10TEL0509245547_2022\n",
      "\tROC UAC:0.9727\n",
      "\tAccuracy:0.9291\n",
      "\tLog Loss:0.3269\n",
      "10TDL0458245240_2022\n",
      "\tROC UAC:0.9947\n",
      "\tAccuracy:0.9727\n",
      "\tLog Loss:0.1988\n",
      "10TEK0503244655_2022\n",
      "\tROC UAC:0.9184\n",
      "\tAccuracy:0.8498\n",
      "\tLog Loss:1.6186\n",
      "10TDL0464245187_2022\n",
      "\tROC UAC:0.9967\n",
      "\tAccuracy:0.9864\n",
      "\tLog Loss:0.0565\n",
      "10TDL0480045075_2020\n",
      "\tROC UAC:0.9895\n",
      "\tAccuracy:0.9976\n",
      "\tLog Loss:0.0502\n",
      "10TDL0480045075_2022\n",
      "\tROC UAC:0.9980\n",
      "\tAccuracy:0.9807\n",
      "\tLog Loss:0.1004\n",
      "10TDL0480045075_2018\n",
      "\tROC UAC:nan\n",
      "\tAccuracy:0.9573\n",
      "\tLog Loss:nan\n",
      "10TEK0500244992_2022\n",
      "\tROC UAC:0.9994\n",
      "\tAccuracy:0.9877\n",
      "\tLog Loss:0.0513\n",
      "10TDL0488245360_2022\n",
      "\tROC UAC:0.9987\n",
      "\tAccuracy:0.9805\n",
      "\tLog Loss:0.1340\n"
     ]
    }
   ],
   "source": [
    "tiles =  list(data.keys())\n",
    "confusion_matrices = []\n",
    "\n",
    "for i in range(len(tiles)):\n",
    "    # separate train and validation sets\n",
    "    test = data[tiles[i]]\n",
    "    train_tiles = tiles[:i] + tiles[i+1:]\n",
    "    train = [data[tile] for tile in train_tiles]\n",
    "    train = pd.concat(train)\n",
    "    \n",
    "    # this is to fix old style labels, does not effect new style labels\n",
    "    train['y'] = train.label.round().astype(int)\n",
    "    test['y'] = test.label.round().astype(int)\n",
    "    \n",
    "    # seperate features and labels\n",
    "    cols = train.drop(['y', 'label', 'UniqueID'], axis=1).columns\n",
    "    X = train[cols]\n",
    "    y = train.y\n",
    "    \n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=params['n_estimators'],\n",
    "        max_features=params['max_features'],\n",
    "        max_depth=params['max_depth'],\n",
    "        oob_score=False,\n",
    "        random_state=np.random.seed(params['np.random.seed'])\n",
    "    ).fit(X, y)\n",
    "    \n",
    "    X_test = test[cols]\n",
    "    y_test = test.y\n",
    "    pred = model.predict(X_test)\n",
    "    try:\n",
    "        roc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    except ValueError:\n",
    "        roc = np.nan\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    try:\n",
    "        loss = log_loss(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    except ValueError:\n",
    "        loss = np.nan\n",
    "    print(tiles[i])\n",
    "    print(f'\\tROC UAC:{roc:.4f}')\n",
    "    print(f'\\tAccuracy:{acc:.4f}')\n",
    "    print(f'\\tLog Loss:{loss:.4f}')\n",
    "    \n",
    "    confusion_matrices.append(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make confusion matrices and save to file without displaying\n",
    "for i, cm in enumerate(confusion_matrices):\n",
    "        d = cm_display1 = ConfusionMatrixDisplay(cm).plot(\n",
    "                colorbar=False,\n",
    "                cmap='binary'\n",
    "                )\n",
    "\n",
    "        ti = ' '.join(tiles[i].split('_'))\n",
    "        plt.title(f'Confusion Matrix, {ti}');\n",
    "        plt.savefig(f'/home/michael/thesis/images/confusion_{tiles[i]}.png')\n",
    "        plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
